{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of Semantic segmentation on onions and tomatoes",
      "provenance": [],
      "collapsed_sections": [
        "4Vk2146Ogil3"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elishatofunmi/Computer-Vision/blob/master/DeepLab/Semantic_segmentation_on_onions_and_tomatoes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "l7EOtpvlLeS0"
      },
      "source": [
        "# Install TensorFlow2 Object Detection Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FNNfNcSELAkU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 888
        },
        "outputId": "8b45ef0b-8323-4685-8e03-8b9945e9336f"
      },
      "source": [
        "#we will utilize the GPU in this tutorial. \n",
        "#TPU configuration is recommended for faster training on larger datsets\n",
        "!pip install -U --pre tensorflow_gpu==\"2.2.0\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: tensorflow_gpu==2.2.0 in /usr/local/lib/python3.6/dist-packages (2.2.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==2.2.0) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==2.2.0) (0.10.0)\n",
            "Collecting tensorboard<2.3.0,>=2.2.0\n",
            "  Using cached https://files.pythonhosted.org/packages/1d/74/0a6fcb206dcc72a6da9a62dd81784bfdbff5fedb099982861dc2219014fb/tensorboard-2.2.2-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==2.2.0) (1.12.1)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==2.2.0) (0.35.1)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==2.2.0) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==2.2.0) (1.32.0)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==2.2.0) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==2.2.0) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==2.2.0) (1.4.1)\n",
            "Collecting tensorflow-estimator<2.3.0,>=2.2.0\n",
            "  Using cached https://files.pythonhosted.org/packages/a4/f5/926ae53d6a226ec0fda5208e0e581cffed895ccc89e36ba76a8e60895b78/tensorflow_estimator-2.2.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==2.2.0) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==2.2.0) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==2.2.0) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==2.2.0) (1.6.3)\n",
            "Requirement already satisfied, skipping upgrade: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==2.2.0) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==2.2.0) (3.13.0)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow_gpu==2.2.0) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow_gpu==2.2.0) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow_gpu==2.2.0) (2.24.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow_gpu==2.2.0) (1.21.2)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow_gpu==2.2.0) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow_gpu==2.2.0) (1.7.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow_gpu==2.2.0) (50.3.0)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow_gpu==2.2.0) (1.7.0)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow_gpu==2.2.0) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow_gpu==2.2.0) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow_gpu==2.2.0) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow_gpu==2.2.0) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow_gpu==2.2.0) (1.25.10)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow_gpu==2.2.0) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow_gpu==2.2.0) (4.1.1)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3.5\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow_gpu==2.2.0) (4.6)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow_gpu==2.2.0) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow_gpu==2.2.0) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow_gpu==2.2.0) (0.4.8)\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement tensorboard<3,>=2.3.0, but you'll have tensorboard 2.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement tensorflow-estimator<2.4.0,>=2.3.0, but you'll have tensorflow-estimator 2.2.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorboard, tensorflow-estimator\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: tensorflow-estimator 2.3.0\n",
            "    Uninstalling tensorflow-estimator-2.3.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
            "Successfully installed tensorboard-2.2.2 tensorflow-estimator-2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ypWGYdPlLRUN",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pathlib\n",
        "\n",
        "# Clone the tensorflow models repository if it doesn't already exist\n",
        "if \"models\" in pathlib.Path.cwd().parts:\n",
        "  while \"models\" in pathlib.Path.cwd().parts:\n",
        "    os.chdir('..')\n",
        "elif not pathlib.Path('models').exists():\n",
        "  !git clone --depth 1 https://github.com/tensorflow/models"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6QPmVBSlLTzM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a743884a-9735-4436-9f49-639fb08737fd"
      },
      "source": [
        "# Install the Object Detection API\n",
        "%%bash\n",
        "cd models/research/\n",
        "protoc object_detection/protos/*.proto --python_out=.\n",
        "cp object_detection/packages/tf2/setup.py .\n",
        "python -m pip install ."
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing /content/models/research\n",
            "Requirement already satisfied: avro-python3 in /usr/local/lib/python3.6/dist-packages (from object-detection==0.1) (1.10.0)\n",
            "Requirement already satisfied: apache-beam in /usr/local/lib/python3.6/dist-packages (from object-detection==0.1) (2.24.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from object-detection==0.1) (7.0.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from object-detection==0.1) (4.2.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from object-detection==0.1) (3.2.2)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (from object-detection==0.1) (0.29.21)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.6/dist-packages (from object-detection==0.1) (0.5.5)\n",
            "Requirement already satisfied: tf-slim in /usr/local/lib/python3.6/dist-packages (from object-detection==0.1) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from object-detection==0.1) (1.15.0)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.6/dist-packages (from object-detection==0.1) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from object-detection==0.1) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from object-detection==0.1) (1.0.5)\n",
            "Requirement already satisfied: tf-models-official in /usr/local/lib/python3.6/dist-packages (from object-detection==0.1) (2.3.0)\n",
            "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from apache-beam->object-detection==0.1) (2.5.8)\n",
            "Requirement already satisfied: grpcio<2,>=1.29.0 in /usr/local/lib/python3.6/dist-packages (from apache-beam->object-detection==0.1) (1.32.0)\n",
            "Requirement already satisfied: oauth2client<4,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from apache-beam->object-detection==0.1) (3.0.0)\n",
            "Requirement already satisfied: numpy<2,>=1.14.3 in /usr/local/lib/python3.6/dist-packages (from apache-beam->object-detection==0.1) (1.18.5)\n",
            "Requirement already satisfied: protobuf<4,>=3.12.2 in /usr/local/lib/python3.6/dist-packages (from apache-beam->object-detection==0.1) (3.13.0)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.6/dist-packages (from apache-beam->object-detection==0.1) (2.8.1)\n",
            "Requirement already satisfied: httplib2<0.18.0,>=0.8 in /usr/local/lib/python3.6/dist-packages (from apache-beam->object-detection==0.1) (0.17.4)\n",
            "Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.6/dist-packages (from apache-beam->object-detection==0.1) (2018.9)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /usr/local/lib/python3.6/dist-packages (from apache-beam->object-detection==0.1) (2.24.0)\n",
            "Requirement already satisfied: typing-extensions<3.8.0,>=3.7.0 in /usr/local/lib/python3.6/dist-packages (from apache-beam->object-detection==0.1) (3.7.4.3)\n",
            "Requirement already satisfied: pyarrow<0.18.0,>=0.15.1; python_version >= \"3.0\" or platform_system != \"Windows\" in /usr/local/lib/python3.6/dist-packages (from apache-beam->object-detection==0.1) (0.17.1)\n",
            "Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in /usr/local/lib/python3.6/dist-packages (from apache-beam->object-detection==0.1) (0.3.1.1)\n",
            "Requirement already satisfied: future<1.0.0,>=0.18.2 in /usr/local/lib/python3.6/dist-packages (from apache-beam->object-detection==0.1) (0.18.2)\n",
            "Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from apache-beam->object-detection==0.1) (3.11.0)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from apache-beam->object-detection==0.1) (1.3.0)\n",
            "Requirement already satisfied: fastavro<0.24,>=0.21.4 in /usr/local/lib/python3.6/dist-packages (from apache-beam->object-detection==0.1) (0.23.6)\n",
            "Requirement already satisfied: mock<3.0.0,>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from apache-beam->object-detection==0.1) (2.0.0)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.6/dist-packages (from apache-beam->object-detection==0.1) (1.7)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->object-detection==0.1) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->object-detection==0.1) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->object-detection==0.1) (0.10.0)\n",
            "Requirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from tf-slim->object-detection==0.1) (0.10.0)\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools->object-detection==0.1) (50.3.0)\n",
            "Requirement already satisfied: tensorflow-model-optimization>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from tf-models-official->object-detection==0.1) (0.5.0)\n",
            "Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.6/dist-packages (from tf-models-official->object-detection==0.1) (1.5.8)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.6/dist-packages (from tf-models-official->object-detection==0.1) (4.4.0.42)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.6/dist-packages (from tf-models-official->object-detection==0.1) (5.4.8)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.6/dist-packages (from tf-models-official->object-detection==0.1) (2.1.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from tf-models-official->object-detection==0.1) (0.7)\n",
            "Requirement already satisfied: py-cpuinfo>=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tf-models-official->object-detection==0.1) (7.0.0)\n",
            "Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.6/dist-packages (from tf-models-official->object-detection==0.1) (1.7.12)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.6/dist-packages (from tf-models-official->object-detection==0.1) (0.3.0)\n",
            "Requirement already satisfied: google-cloud-bigquery>=0.31.0 in /usr/local/lib/python3.6/dist-packages (from tf-models-official->object-detection==0.1) (1.21.0)\n",
            "Requirement already satisfied: tensorflow>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tf-models-official->object-detection==0.1) (2.3.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from tf-models-official->object-detection==0.1) (0.1.91)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from tf-models-official->object-detection==0.1) (3.13)\n",
            "Requirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-models-official->object-detection==0.1) (0.9.0)\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.6/dist-packages (from tf-models-official->object-detection==0.1) (0.8.3)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.6/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam->object-detection==0.1) (0.6.2)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client<4,>=2.0.1->apache-beam->object-detection==0.1) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client<4,>=2.0.1->apache-beam->object-detection==0.1) (0.2.8)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client<4,>=2.0.1->apache-beam->object-detection==0.1) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam->object-detection==0.1) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam->object-detection==0.1) (1.25.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam->object-detection==0.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam->object-detection==0.1) (2.10)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python3.6/dist-packages (from mock<3.0.0,>=1.0.1->apache-beam->object-detection==0.1) (5.5.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-model-optimization>=0.2.1->tf-models-official->object-detection==0.1) (0.1.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-official->object-detection==0.1) (4.41.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-official->object-detection==0.1) (4.0.1)\n",
            "Requirement already satisfied: slugify in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-official->object-detection==0.1) (0.0.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-official->object-detection==0.1) (1.12.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-official->object-detection==0.1) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-official->object-detection==0.1) (0.24.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-official->object-detection==0.1) (2.3)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-official->object-detection==0.1) (20.2.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.6.7->tf-models-official->object-detection==0.1) (0.0.4)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.6.7->tf-models-official->object-detection==0.1) (3.0.1)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.6.7->tf-models-official->object-detection==0.1) (1.21.2)\n",
            "Requirement already satisfied: google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.6/dist-packages (from google-cloud-bigquery>=0.31.0->tf-models-official->object-detection==0.1) (0.4.1)\n",
            "Requirement already satisfied: google-cloud-core<2.0dev,>=1.0.3 in /usr/local/lib/python3.6/dist-packages (from google-cloud-bigquery>=0.31.0->tf-models-official->object-detection==0.1) (1.0.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (0.2.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (1.6.3)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (2.10.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (0.35.1)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (1.1.2)\n",
            "Collecting tensorboard<3,>=2.3.0\n",
            "  Using cached https://files.pythonhosted.org/packages/e9/1b/6a420d7e6ba431cf3d51b2a5bfa06a958c4141e3189385963dc7f6fbffb6/tensorboard-2.3.0-py3-none-any.whl\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (0.3.3)\n",
            "Collecting tensorflow-estimator<2.4.0,>=2.3.0\n",
            "  Using cached https://files.pythonhosted.org/packages/e9/ed/5853ec0ae380cba4588eab1524e18ece1583b65f7ae0e97321f5ff9dfd60/tensorflow_estimator-2.3.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (3.3.0)\n",
            "Requirement already satisfied: typeguard in /usr/local/lib/python3.6/dist-packages (from tensorflow-addons->tf-models-official->object-detection==0.1) (2.7.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official->object-detection==0.1) (1.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow-datasets->tf-models-official->object-detection==0.1) (1.52.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.6.7->tf-models-official->object-detection==0.1) (4.1.1)\n",
            "Requirement already satisfied: google-api-core<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-core<2.0dev,>=1.0.3->google-cloud-bigquery>=0.31.0->tf-models-official->object-detection==0.1) (1.16.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (3.2.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (1.7.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (0.4.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (1.7.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (3.1.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (3.1.0)\n",
            "Building wheels for collected packages: object-detection\n",
            "  Building wheel for object-detection (setup.py): started\n",
            "  Building wheel for object-detection (setup.py): finished with status 'done'\n",
            "  Created wheel for object-detection: filename=object_detection-0.1-cp36-none-any.whl size=1577697 sha256=4198b0c5d6d8df6872a77a6f12d479e5e9145c4b4d0d75232be601681bc77dbe\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5jyy4mdm/wheels/94/49/4b/39b051683087a22ef7e80ec52152a27249d1a644ccf4e442ea\n",
            "Successfully built object-detection\n",
            "Installing collected packages: object-detection, tensorboard, tensorflow-estimator\n",
            "  Found existing installation: object-detection 0.1\n",
            "    Uninstalling object-detection-0.1:\n",
            "      Successfully uninstalled object-detection-0.1\n",
            "  Found existing installation: tensorboard 2.2.2\n",
            "    Uninstalling tensorboard-2.2.2:\n",
            "      Successfully uninstalled tensorboard-2.2.2\n",
            "  Found existing installation: tensorflow-estimator 2.2.0\n",
            "    Uninstalling tensorflow-estimator-2.2.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0\n",
            "Successfully installed object-detection-0.1 tensorboard-2.3.0 tensorflow-estimator-2.3.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "ERROR: tensorflow-gpu 2.2.0 has requirement tensorboard<2.3.0,>=2.2.0, but you'll have tensorboard 2.3.0 which is incompatible.\n",
            "ERROR: tensorflow-gpu 2.2.0 has requirement tensorflow-estimator<2.3.0,>=2.2.0, but you'll have tensorflow-estimator 2.3.0 which is incompatible.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6HHXZrSopwri",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7a9fc1e7-b918-4c1e-96fe-c450e3c64f17"
      },
      "source": [
        "#commence temporary keras fix for exporting efficientDet - this will inevitably be fixed and removed from the notebook in the coming days\n",
        "%cat /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "#Roboflow: we are making a tiny change to the keras utils so we can export weights!\n",
            "\n",
            "# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n",
            "#\n",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "# you may not use this file except in compliance with the License.\n",
            "# You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "# ==============================================================================\n",
            "\"\"\"TensorFlow-related utilities.\"\"\"\n",
            "from __future__ import absolute_import\n",
            "from __future__ import division\n",
            "from __future__ import print_function\n",
            "\n",
            "import copy\n",
            "import numpy as np\n",
            "import six\n",
            "\n",
            "from tensorflow.python.data.experimental.ops import cardinality\n",
            "from tensorflow.python.eager import context\n",
            "from tensorflow.python.framework import composite_tensor\n",
            "from tensorflow.python.framework import ops\n",
            "from tensorflow.python.framework import smart_cond as smart_module\n",
            "from tensorflow.python.framework import tensor_shape\n",
            "from tensorflow.python.framework import tensor_spec\n",
            "from tensorflow.python.framework import tensor_util\n",
            "from tensorflow.python.framework import type_spec\n",
            "from tensorflow.python.keras import backend as K\n",
            "from tensorflow.python.ops import control_flow_ops\n",
            "from tensorflow.python.ops import math_ops\n",
            "from tensorflow.python.ops import variables\n",
            "from tensorflow.python.util import nest\n",
            "from tensorflow.python.util import object_identity\n",
            "from tensorflow.python.util import tf_contextlib\n",
            "\n",
            "\n",
            "def smart_cond(pred, true_fn=None, false_fn=None, name=None):\n",
            "  \"\"\"Return either `true_fn()` if predicate `pred` is true else `false_fn()`.\n",
            "\n",
            "  If `pred` is a bool or has a constant value, we return either `true_fn()`\n",
            "  or `false_fn()`, otherwise we use `tf.cond` to dynamically route to both.\n",
            "\n",
            "  Arguments:\n",
            "    pred: A scalar determining whether to return the result of `true_fn` or\n",
            "      `false_fn`.\n",
            "    true_fn: The callable to be performed if pred is true.\n",
            "    false_fn: The callable to be performed if pred is false.\n",
            "    name: Optional name prefix when using `tf.cond`.\n",
            "\n",
            "  Returns:\n",
            "    Tensors returned by the call to either `true_fn` or `false_fn`.\n",
            "\n",
            "  Raises:\n",
            "    TypeError: If `true_fn` or `false_fn` is not callable.\n",
            "  \"\"\"\n",
            "  if isinstance(pred, variables.Variable):\n",
            "    return control_flow_ops.cond(\n",
            "        pred, true_fn=true_fn, false_fn=false_fn, name=name)\n",
            "  return smart_module.smart_cond(\n",
            "      pred, true_fn=true_fn, false_fn=false_fn, name=name)\n",
            "\n",
            "\n",
            "def constant_value(pred):\n",
            "  \"\"\"Return the bool value for `pred`, or None if `pred` had a dynamic value.\n",
            "\n",
            "  Arguments:\n",
            "    pred: A scalar, either a Python bool or a TensorFlow boolean variable\n",
            "      or tensor, or the Python integer 1 or 0.\n",
            "\n",
            "  Returns:\n",
            "    True or False if `pred` has a constant boolean value, None otherwise.\n",
            "\n",
            "  Raises:\n",
            "    TypeError: If `pred` is not a Variable, Tensor or bool, or Python\n",
            "      integer 1 or 0.\n",
            "  \"\"\"\n",
            "  # Allow integer booleans.\n",
            "  if isinstance(pred, int):\n",
            "    if pred == 1:\n",
            "      pred = True\n",
            "    elif pred == 0:\n",
            "      pred = False\n",
            "\n",
            "  if isinstance(pred, variables.Variable):\n",
            "    return None\n",
            "  return smart_module.smart_constant_value(pred)\n",
            "\n",
            "\n",
            "def is_tensor_or_tensor_list(v):\n",
            "  v = nest.flatten(v)\n",
            "  if v and isinstance(v[0], ops.Tensor):\n",
            "    return True\n",
            "  else:\n",
            "    return False\n",
            "\n",
            "\n",
            "def get_reachable_from_inputs(inputs, targets=None):\n",
            "  \"\"\"Returns the set of tensors/ops reachable from `inputs`.\n",
            "\n",
            "  Stops if all targets have been found (target is optional).\n",
            "\n",
            "  Only valid in Symbolic mode, not Eager mode.\n",
            "\n",
            "  Args:\n",
            "    inputs: List of tensors.\n",
            "    targets: List of tensors.\n",
            "\n",
            "  Returns:\n",
            "    A set of tensors reachable from the inputs (includes the inputs themselves).\n",
            "  \"\"\"\n",
            "  inputs = nest.flatten(inputs, expand_composites=True)\n",
            "  reachable = object_identity.ObjectIdentitySet(inputs)\n",
            "  if targets:\n",
            "    remaining_targets = object_identity.ObjectIdentitySet(nest.flatten(targets))\n",
            "  queue = inputs[:]\n",
            "\n",
            "  while queue:\n",
            "    x = queue.pop()\n",
            "    if isinstance(x, tuple(_user_convertible_tensor_types)):\n",
            "      # Can't find consumers of user-specific types.\n",
            "      continue\n",
            "\n",
            "    if isinstance(x, ops.Operation):\n",
            "      outputs = x.outputs[:] or []\n",
            "      outputs += x._control_outputs  # pylint: disable=protected-access\n",
            "    elif isinstance(x, variables.Variable):\n",
            "      try:\n",
            "        outputs = [x.op]\n",
            "      except AttributeError:\n",
            "        # Variables can be created in an Eager context.\n",
            "        outputs = []\n",
            "    elif tensor_util.is_tensor(x):\n",
            "      outputs = x.consumers()\n",
            "    else:\n",
            "      if not isinstance(x, str):\n",
            "        raise TypeError('Expected Operation, Variable, or Tensor, got ' + str(x))\n",
            "\n",
            "    for y in outputs:\n",
            "      if y not in reachable:\n",
            "        reachable.add(y)\n",
            "        if targets:\n",
            "          remaining_targets.discard(y)\n",
            "        queue.insert(0, y)\n",
            "\n",
            "    if targets and not remaining_targets:\n",
            "      return reachable\n",
            "\n",
            "  return reachable\n",
            "\n",
            "\n",
            "# This function needs access to private functions of `nest`.\n",
            "#  pylint: disable=protected-access\n",
            "def map_structure_with_atomic(is_atomic_fn, map_fn, nested):\n",
            "  \"\"\"Maps the atomic elements of a nested structure.\n",
            "\n",
            "  Arguments:\n",
            "    is_atomic_fn: A function that determines if an element of `nested` is\n",
            "      atomic.\n",
            "    map_fn: The function to apply to atomic elements of `nested`.\n",
            "    nested: A nested structure.\n",
            "\n",
            "  Returns:\n",
            "    The nested structure, with atomic elements mapped according to `map_fn`.\n",
            "\n",
            "  Raises:\n",
            "    ValueError: If an element that is neither atomic nor a sequence is\n",
            "      encountered.\n",
            "  \"\"\"\n",
            "  if is_atomic_fn(nested):\n",
            "    return map_fn(nested)\n",
            "\n",
            "  # Recursively convert.\n",
            "  if not nest.is_sequence(nested):\n",
            "    raise ValueError(\n",
            "        'Received non-atomic and non-sequence element: {}'.format(nested))\n",
            "  if nest._is_mapping(nested):\n",
            "    values = [nested[k] for k in nest._sorted(nested)]\n",
            "  else:\n",
            "    values = nested\n",
            "  mapped_values = [\n",
            "      map_structure_with_atomic(is_atomic_fn, map_fn, ele) for ele in values\n",
            "  ]\n",
            "  return nest._sequence_like(nested, mapped_values)\n",
            "\n",
            "\n",
            "#  pylint: enable=protected-access\n",
            "\n",
            "\n",
            "def convert_shapes(input_shape, to_tuples=True):\n",
            "  \"\"\"Converts nested shape representations to desired format.\n",
            "\n",
            "  Performs:\n",
            "\n",
            "  TensorShapes -> tuples if `to_tuples=True`.\n",
            "  tuples of int or None -> TensorShapes if `to_tuples=False`.\n",
            "\n",
            "  Valid objects to be converted are:\n",
            "  - TensorShapes\n",
            "  - tuples with elements of type int or None.\n",
            "  - ints\n",
            "  - None\n",
            "\n",
            "  Arguments:\n",
            "    input_shape: A nested structure of objects to be converted to TensorShapes.\n",
            "    to_tuples: If `True`, converts all TensorShape to tuples. Otherwise converts\n",
            "      all tuples representing shapes to TensorShapes.\n",
            "\n",
            "  Returns:\n",
            "    Nested structure of shapes in desired format.\n",
            "\n",
            "  Raises:\n",
            "    ValueError: when the input tensor shape can't be converted to tuples, eg\n",
            "      unknown tensor shape.\n",
            "  \"\"\"\n",
            "\n",
            "  def _is_shape_component(value):\n",
            "    return value is None or isinstance(value, (int, tensor_shape.Dimension))\n",
            "\n",
            "  def _is_atomic_shape(input_shape):\n",
            "    # Ex: TensorShape or (None, 10, 32) or 5 or `None`\n",
            "    if _is_shape_component(input_shape):\n",
            "      return True\n",
            "    if isinstance(input_shape, tensor_shape.TensorShape):\n",
            "      return True\n",
            "    if (isinstance(input_shape, (tuple, list)) and\n",
            "        all(_is_shape_component(ele) for ele in input_shape)):\n",
            "      return True\n",
            "    return False\n",
            "\n",
            "  def _convert_shape(input_shape):\n",
            "    input_shape = tensor_shape.TensorShape(input_shape)\n",
            "    if to_tuples:\n",
            "      input_shape = tuple(input_shape.as_list())\n",
            "    return input_shape\n",
            "\n",
            "  return map_structure_with_atomic(_is_atomic_shape, _convert_shape,\n",
            "                                   input_shape)\n",
            "\n",
            "\n",
            "class ListWrapper(object):\n",
            "  \"\"\"A wrapper for lists to be treated as elements for `nest`.\"\"\"\n",
            "\n",
            "  def __init__(self, list_to_wrap):\n",
            "    self._list = list_to_wrap\n",
            "\n",
            "  def as_list(self):\n",
            "    return self._list\n",
            "\n",
            "\n",
            "def convert_inner_node_data(nested, wrap=False):\n",
            "  \"\"\"Either wraps or unwraps innermost node data lists in `ListWrapper` objects.\n",
            "\n",
            "  Arguments:\n",
            "    nested: A nested data structure.\n",
            "    wrap: If `True`, wrap innermost lists in `ListWrapper` objects. If `False`,\n",
            "      unwraps `ListWrapper` objects into lists.\n",
            "\n",
            "  Returns:\n",
            "    Structure of same type as nested, with lists wrapped/unwrapped.\n",
            "  \"\"\"\n",
            "\n",
            "  def _is_serialized_node_data(nested):\n",
            "    # Node data can be of form `[layer_name, node_id, tensor_id]` or\n",
            "    # `[layer_name, node_id, tensor_id, kwargs]`.\n",
            "    if (isinstance(nested, list) and (len(nested) in [3, 4]) and\n",
            "        isinstance(nested[0], six.string_types)):\n",
            "      return True\n",
            "    return False\n",
            "\n",
            "  def _is_atomic_nested(nested):\n",
            "    \"\"\"Returns `True` if `nested` is a list representing node data.\"\"\"\n",
            "    if isinstance(nested, ListWrapper):\n",
            "      return True\n",
            "    if _is_serialized_node_data(nested):\n",
            "      return True\n",
            "    return not nest.is_sequence(nested)\n",
            "\n",
            "  def _convert_object_or_list(nested):\n",
            "    \"\"\"Convert b/t `ListWrapper` object and list representations.\"\"\"\n",
            "    if wrap:\n",
            "      if isinstance(nested, ListWrapper):\n",
            "        return nested\n",
            "      if _is_serialized_node_data(nested):\n",
            "        return ListWrapper(nested)\n",
            "      return nested\n",
            "    else:\n",
            "      if isinstance(nested, ListWrapper):\n",
            "        return nested.as_list()\n",
            "      return nested\n",
            "\n",
            "  return map_structure_with_atomic(_is_atomic_nested, _convert_object_or_list,\n",
            "                                   nested)\n",
            "\n",
            "\n",
            "def shape_type_conversion(fn):\n",
            "  \"\"\"Decorator that handles tuple/TensorShape conversion.\n",
            "\n",
            "  Used in `compute_output_shape` and `build`.\n",
            "\n",
            "  Arguments:\n",
            "    fn: function to wrap.\n",
            "\n",
            "  Returns:\n",
            "    Wrapped function.\n",
            "  \"\"\"\n",
            "\n",
            "  def wrapper(instance, input_shape):\n",
            "    # Pass shapes as tuples to `fn`\n",
            "    # This preserves compatibility with external Keras.\n",
            "    if input_shape is not None:\n",
            "      input_shape = convert_shapes(input_shape, to_tuples=True)\n",
            "    output_shape = fn(instance, input_shape)\n",
            "    # Return shapes from `fn` as TensorShapes.\n",
            "    if output_shape is not None:\n",
            "      output_shape = convert_shapes(output_shape, to_tuples=False)\n",
            "    return output_shape\n",
            "\n",
            "  return wrapper\n",
            "\n",
            "\n",
            "def are_all_symbolic_tensors(tensors):\n",
            "  return all(is_symbolic_tensor(tensor) for tensor in tensors)\n",
            "\n",
            "\n",
            "_user_convertible_tensor_types = set()\n",
            "\n",
            "\n",
            "def is_symbolic_tensor(tensor):\n",
            "  \"\"\"Returns whether a tensor is symbolic (from a TF graph) or an eager tensor.\n",
            "\n",
            "  A Variable can be seen as either: it is considered symbolic\n",
            "  when we are in a graph scope, and eager when we are in an eager scope.\n",
            "\n",
            "  Arguments:\n",
            "    tensor: A tensor instance to test.\n",
            "\n",
            "  Returns:\n",
            "    True for symbolic tensors, False for eager tensors.\n",
            "  \"\"\"\n",
            "  if isinstance(tensor, tuple(_user_convertible_tensor_types)):\n",
            "    tensor = ops.convert_to_tensor_or_composite(tensor)\n",
            "  if isinstance(tensor, variables.Variable):\n",
            "    # Variables that are output of a Keras Layer in Functional API mode\n",
            "    # should be considered symbolic.\n",
            "    # TODO(omalleyt): We need a better way to check this in order to\n",
            "    # enable `run_eagerly=True` for Models containing Layers that\n",
            "    # return Variables as outputs.\n",
            "    return (getattr(tensor, '_keras_history', False) or\n",
            "            not context.executing_eagerly())\n",
            "  if isinstance(tensor, composite_tensor.CompositeTensor):\n",
            "    component_tensors = nest.flatten(tensor, expand_composites=True)\n",
            "    return any(hasattr(t, 'graph') for t in component_tensors)\n",
            "  if isinstance(tensor, ops.Tensor):\n",
            "    return hasattr(tensor, 'graph')\n",
            "  return False\n",
            "\n",
            "\n",
            "def register_symbolic_tensor_type(cls):\n",
            "  \"\"\"Allows users to specify types regarded as symbolic `Tensor`s.\n",
            "\n",
            "  Used in conjunction with `tf.register_tensor_conversion_function`, calling\n",
            "  `tf.keras.utils.register_symbolic_tensor_type(cls)` allows non-`Tensor`\n",
            "  objects to be plumbed through Keras layers.\n",
            "\n",
            "  Example:\n",
            "\n",
            "  ```python\n",
            "  # One-time setup.\n",
            "  class Foo(object):\n",
            "    def __init__(self, input_):\n",
            "      self._input = input_\n",
            "    def value(self):\n",
            "      return tf.constant(42.)\n",
            "\n",
            "  tf.register_tensor_conversion_function(\n",
            "      Foo, lambda x, *args, **kwargs: x.value())\n",
            "\n",
            "  tf.keras.utils.register_symbolic_tensor_type(Foo)\n",
            "\n",
            "  # User-land.\n",
            "  layer = tf.keras.layers.Lambda(lambda input_: Foo(input_))\n",
            "  ```\n",
            "\n",
            "  Arguments:\n",
            "    cls: A `class` type which shall be regarded as a symbolic `Tensor`.\n",
            "  \"\"\"\n",
            "  global _user_convertible_tensor_types\n",
            "  _user_convertible_tensor_types.add(cls)\n",
            "\n",
            "\n",
            "def type_spec_from_value(value):\n",
            "  \"\"\"Grab type_spec without converting array-likes to tensors.\"\"\"\n",
            "  if isinstance(value, composite_tensor.CompositeTensor):\n",
            "    return value._type_spec  # pylint: disable=protected-access\n",
            "  # Get a TensorSpec for array-like data without\n",
            "  # converting the data to a Tensor\n",
            "  if hasattr(value, 'shape') and hasattr(value, 'dtype'):\n",
            "    return tensor_spec.TensorSpec(value.shape, value.dtype)\n",
            "  else:\n",
            "    return type_spec.type_spec_from_value(value)\n",
            "\n",
            "\n",
            "def is_tensor_or_variable(x):\n",
            "  return tensor_util.is_tensor(x) or isinstance(x, variables.Variable)\n",
            "\n",
            "\n",
            "def assert_no_legacy_layers(layers):\n",
            "  \"\"\"Prevent tf.layers.Layers from being used with Keras.\n",
            "\n",
            "  Certain legacy layers inherit from their keras analogs; however they are\n",
            "  not supported with keras and can lead to subtle and hard to diagnose bugs.\n",
            "\n",
            "  Args:\n",
            "    layers: A list of layers to check\n",
            "\n",
            "  Raises:\n",
            "    TypeError: If any elements of layers are tf.layers.Layers\n",
            "  \"\"\"\n",
            "\n",
            "  # isinstance check for tf.layers.Layer introduces a circular dependency.\n",
            "  legacy_layers = [l for l in layers if getattr(l, '_is_legacy_layer', None)]\n",
            "  if legacy_layers:\n",
            "    layer_str = '\\n'.join('  ' + str(l) for l in legacy_layers)\n",
            "    raise TypeError(\n",
            "        'The following are legacy tf.layers.Layers:\\n{}\\nTo use keras as a '\n",
            "        'framework (for instance using the Network, Model, or Sequential '\n",
            "        'classes), please use the tf.keras.layers implementation instead. '\n",
            "        '(Or, if writing custom layers, subclass from tf.keras.layers rather '\n",
            "        'than tf.layers)'.format(layer_str))\n",
            "\n",
            "\n",
            "@tf_contextlib.contextmanager\n",
            "def maybe_init_scope(layer):\n",
            "  \"\"\"Open an `init_scope` if in V2 mode and using the keras graph.\n",
            "\n",
            "  Arguments:\n",
            "    layer: The Layer/Model that is currently active.\n",
            "\n",
            "  Yields:\n",
            "    None\n",
            "  \"\"\"\n",
            "  # Don't open an init_scope in V1 mode or when using legacy tf.layers.\n",
            "  if (ops.executing_eagerly_outside_functions() and\n",
            "      getattr(layer, '_keras_style', True)):\n",
            "    with ops.init_scope():\n",
            "      yield\n",
            "  else:\n",
            "    yield\n",
            "\n",
            "\n",
            "@tf_contextlib.contextmanager\n",
            "def graph_context_for_symbolic_tensors(*args, **kwargs):\n",
            "  \"\"\"Returns graph context manager if any of the inputs is a symbolic tensor.\"\"\"\n",
            "  if any(is_symbolic_tensor(v) for v in list(args) + list(kwargs.values())):\n",
            "    with K.get_graph().as_default():\n",
            "      yield\n",
            "  else:\n",
            "    yield\n",
            "\n",
            "\n",
            "def dataset_is_infinite(dataset):\n",
            "  \"\"\"True if the passed dataset is infinite.\"\"\"\n",
            "  if ops.executing_eagerly_outside_functions():\n",
            "    return math_ops.equal(\n",
            "        cardinality.cardinality(dataset), cardinality.INFINITE)\n",
            "  else:\n",
            "    dataset_size = K.get_session().run(cardinality.cardinality(dataset))\n",
            "    return dataset_size == cardinality.INFINITE\n",
            "\n",
            "\n",
            "def get_tensor_spec(t, dynamic_batch=False, name=None):\n",
            "  \"\"\"Returns a `TensorSpec` given a single `Tensor` or `TensorSpec`.\"\"\"\n",
            "  if isinstance(t, type_spec.TypeSpec):\n",
            "    spec = t\n",
            "  elif isinstance(t, composite_tensor.CompositeTensor):\n",
            "    # TODO(b/148821952): Should these specs have a name attr?\n",
            "    spec = t._type_spec  # pylint: disable=protected-access\n",
            "  elif hasattr(t, 'shape') and hasattr(t, 'dtype'):\n",
            "    spec = tensor_spec.TensorSpec(shape=t.shape, dtype=t.dtype, name=name)\n",
            "  else:\n",
            "    return None  # Allow non-Tensors to pass through.\n",
            "\n",
            "  if not dynamic_batch:\n",
            "    return spec\n",
            "\n",
            "  dynamic_batch_spec = copy.deepcopy(spec)\n",
            "  # RaggedTensorSpec only has a private _shape.\n",
            "  shape = dynamic_batch_spec._shape.as_list()  # pylint: disable=protected-access\n",
            "  if shape:\n",
            "    shape[0] = None\n",
            "    dynamic_batch_spec._shape = tensor_shape.TensorShape(shape)  # pylint: disable=protected-access\n",
            "  return dynamic_batch_spec\n",
            "\n",
            "\n",
            "def to_numpy_or_python_type(tensors):\n",
            "  \"\"\"Converts a structure of `Tensor`s to `NumPy` arrays or Python scalar types.\n",
            "\n",
            "  For each tensor, it calls `tensor.numpy()`. If the result is a scalar value,\n",
            "  it converts it to a Python type, such as a float or int, by calling\n",
            "  `result.item()`.\n",
            "\n",
            "  Numpy scalars are converted, as Python types are often more convenient to deal\n",
            "  with. This is especially useful for bfloat16 Numpy scalars, which don't\n",
            "  support as many operations as other Numpy values.\n",
            "\n",
            "  Args:\n",
            "    tensors: A structure of tensors.\n",
            "\n",
            "  Returns:\n",
            "    `tensors`, but scalar tensors are converted to Python types and non-scalar\n",
            "    tensors are converted to Numpy arrays.\n",
            "  \"\"\"\n",
            "  def _to_single_numpy_or_python_type(t):\n",
            "    if isinstance(t, ops.Tensor):\n",
            "      x = t.numpy()\n",
            "      return x.item() if np.ndim(x) == 0 else x\n",
            "    return t  # Don't turn ragged or sparse tensors to NumPy.\n",
            "\n",
            "  return nest.map_structure(_to_single_numpy_or_python_type, tensors)"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OGE37OTsqFdr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7896d61d-6619-4733-85ec-b5dd3ef187e7"
      },
      "source": [
        "%%writefile /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py\n",
        "\n",
        "\n",
        "#Roboflow: we are making a tiny change to the keras utils so we can export weights!\n",
        "\n",
        "# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\"\"\"TensorFlow-related utilities.\"\"\"\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import copy\n",
        "import numpy as np\n",
        "import six\n",
        "\n",
        "from tensorflow.python.data.experimental.ops import cardinality\n",
        "from tensorflow.python.eager import context\n",
        "from tensorflow.python.framework import composite_tensor\n",
        "from tensorflow.python.framework import ops\n",
        "from tensorflow.python.framework import smart_cond as smart_module\n",
        "from tensorflow.python.framework import tensor_shape\n",
        "from tensorflow.python.framework import tensor_spec\n",
        "from tensorflow.python.framework import tensor_util\n",
        "from tensorflow.python.framework import type_spec\n",
        "from tensorflow.python.keras import backend as K\n",
        "from tensorflow.python.ops import control_flow_ops\n",
        "from tensorflow.python.ops import math_ops\n",
        "from tensorflow.python.ops import variables\n",
        "from tensorflow.python.util import nest\n",
        "from tensorflow.python.util import object_identity\n",
        "from tensorflow.python.util import tf_contextlib\n",
        "\n",
        "\n",
        "def smart_cond(pred, true_fn=None, false_fn=None, name=None):\n",
        "  \"\"\"Return either `true_fn()` if predicate `pred` is true else `false_fn()`.\n",
        "\n",
        "  If `pred` is a bool or has a constant value, we return either `true_fn()`\n",
        "  or `false_fn()`, otherwise we use `tf.cond` to dynamically route to both.\n",
        "\n",
        "  Arguments:\n",
        "    pred: A scalar determining whether to return the result of `true_fn` or\n",
        "      `false_fn`.\n",
        "    true_fn: The callable to be performed if pred is true.\n",
        "    false_fn: The callable to be performed if pred is false.\n",
        "    name: Optional name prefix when using `tf.cond`.\n",
        "\n",
        "  Returns:\n",
        "    Tensors returned by the call to either `true_fn` or `false_fn`.\n",
        "\n",
        "  Raises:\n",
        "    TypeError: If `true_fn` or `false_fn` is not callable.\n",
        "  \"\"\"\n",
        "  if isinstance(pred, variables.Variable):\n",
        "    return control_flow_ops.cond(\n",
        "        pred, true_fn=true_fn, false_fn=false_fn, name=name)\n",
        "  return smart_module.smart_cond(\n",
        "      pred, true_fn=true_fn, false_fn=false_fn, name=name)\n",
        "\n",
        "\n",
        "def constant_value(pred):\n",
        "  \"\"\"Return the bool value for `pred`, or None if `pred` had a dynamic value.\n",
        "\n",
        "  Arguments:\n",
        "    pred: A scalar, either a Python bool or a TensorFlow boolean variable\n",
        "      or tensor, or the Python integer 1 or 0.\n",
        "\n",
        "  Returns:\n",
        "    True or False if `pred` has a constant boolean value, None otherwise.\n",
        "\n",
        "  Raises:\n",
        "    TypeError: If `pred` is not a Variable, Tensor or bool, or Python\n",
        "      integer 1 or 0.\n",
        "  \"\"\"\n",
        "  # Allow integer booleans.\n",
        "  if isinstance(pred, int):\n",
        "    if pred == 1:\n",
        "      pred = True\n",
        "    elif pred == 0:\n",
        "      pred = False\n",
        "\n",
        "  if isinstance(pred, variables.Variable):\n",
        "    return None\n",
        "  return smart_module.smart_constant_value(pred)\n",
        "\n",
        "\n",
        "def is_tensor_or_tensor_list(v):\n",
        "  v = nest.flatten(v)\n",
        "  if v and isinstance(v[0], ops.Tensor):\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "\n",
        "def get_reachable_from_inputs(inputs, targets=None):\n",
        "  \"\"\"Returns the set of tensors/ops reachable from `inputs`.\n",
        "\n",
        "  Stops if all targets have been found (target is optional).\n",
        "\n",
        "  Only valid in Symbolic mode, not Eager mode.\n",
        "\n",
        "  Args:\n",
        "    inputs: List of tensors.\n",
        "    targets: List of tensors.\n",
        "\n",
        "  Returns:\n",
        "    A set of tensors reachable from the inputs (includes the inputs themselves).\n",
        "  \"\"\"\n",
        "  inputs = nest.flatten(inputs, expand_composites=True)\n",
        "  reachable = object_identity.ObjectIdentitySet(inputs)\n",
        "  if targets:\n",
        "    remaining_targets = object_identity.ObjectIdentitySet(nest.flatten(targets))\n",
        "  queue = inputs[:]\n",
        "\n",
        "  while queue:\n",
        "    x = queue.pop()\n",
        "    if isinstance(x, tuple(_user_convertible_tensor_types)):\n",
        "      # Can't find consumers of user-specific types.\n",
        "      continue\n",
        "\n",
        "    if isinstance(x, ops.Operation):\n",
        "      outputs = x.outputs[:] or []\n",
        "      outputs += x._control_outputs  # pylint: disable=protected-access\n",
        "    elif isinstance(x, variables.Variable):\n",
        "      try:\n",
        "        outputs = [x.op]\n",
        "      except AttributeError:\n",
        "        # Variables can be created in an Eager context.\n",
        "        outputs = []\n",
        "    elif tensor_util.is_tensor(x):\n",
        "      outputs = x.consumers()\n",
        "    else:\n",
        "      if not isinstance(x, str):\n",
        "        raise TypeError('Expected Operation, Variable, or Tensor, got ' + str(x))\n",
        "\n",
        "    for y in outputs:\n",
        "      if y not in reachable:\n",
        "        reachable.add(y)\n",
        "        if targets:\n",
        "          remaining_targets.discard(y)\n",
        "        queue.insert(0, y)\n",
        "\n",
        "    if targets and not remaining_targets:\n",
        "      return reachable\n",
        "\n",
        "  return reachable\n",
        "\n",
        "\n",
        "# This function needs access to private functions of `nest`.\n",
        "#  pylint: disable=protected-access\n",
        "def map_structure_with_atomic(is_atomic_fn, map_fn, nested):\n",
        "  \"\"\"Maps the atomic elements of a nested structure.\n",
        "\n",
        "  Arguments:\n",
        "    is_atomic_fn: A function that determines if an element of `nested` is\n",
        "      atomic.\n",
        "    map_fn: The function to apply to atomic elements of `nested`.\n",
        "    nested: A nested structure.\n",
        "\n",
        "  Returns:\n",
        "    The nested structure, with atomic elements mapped according to `map_fn`.\n",
        "\n",
        "  Raises:\n",
        "    ValueError: If an element that is neither atomic nor a sequence is\n",
        "      encountered.\n",
        "  \"\"\"\n",
        "  if is_atomic_fn(nested):\n",
        "    return map_fn(nested)\n",
        "\n",
        "  # Recursively convert.\n",
        "  if not nest.is_sequence(nested):\n",
        "    raise ValueError(\n",
        "        'Received non-atomic and non-sequence element: {}'.format(nested))\n",
        "  if nest._is_mapping(nested):\n",
        "    values = [nested[k] for k in nest._sorted(nested)]\n",
        "  else:\n",
        "    values = nested\n",
        "  mapped_values = [\n",
        "      map_structure_with_atomic(is_atomic_fn, map_fn, ele) for ele in values\n",
        "  ]\n",
        "  return nest._sequence_like(nested, mapped_values)\n",
        "\n",
        "\n",
        "#  pylint: enable=protected-access\n",
        "\n",
        "\n",
        "def convert_shapes(input_shape, to_tuples=True):\n",
        "  \"\"\"Converts nested shape representations to desired format.\n",
        "\n",
        "  Performs:\n",
        "\n",
        "  TensorShapes -> tuples if `to_tuples=True`.\n",
        "  tuples of int or None -> TensorShapes if `to_tuples=False`.\n",
        "\n",
        "  Valid objects to be converted are:\n",
        "  - TensorShapes\n",
        "  - tuples with elements of type int or None.\n",
        "  - ints\n",
        "  - None\n",
        "\n",
        "  Arguments:\n",
        "    input_shape: A nested structure of objects to be converted to TensorShapes.\n",
        "    to_tuples: If `True`, converts all TensorShape to tuples. Otherwise converts\n",
        "      all tuples representing shapes to TensorShapes.\n",
        "\n",
        "  Returns:\n",
        "    Nested structure of shapes in desired format.\n",
        "\n",
        "  Raises:\n",
        "    ValueError: when the input tensor shape can't be converted to tuples, eg\n",
        "      unknown tensor shape.\n",
        "  \"\"\"\n",
        "\n",
        "  def _is_shape_component(value):\n",
        "    return value is None or isinstance(value, (int, tensor_shape.Dimension))\n",
        "\n",
        "  def _is_atomic_shape(input_shape):\n",
        "    # Ex: TensorShape or (None, 10, 32) or 5 or `None`\n",
        "    if _is_shape_component(input_shape):\n",
        "      return True\n",
        "    if isinstance(input_shape, tensor_shape.TensorShape):\n",
        "      return True\n",
        "    if (isinstance(input_shape, (tuple, list)) and\n",
        "        all(_is_shape_component(ele) for ele in input_shape)):\n",
        "      return True\n",
        "    return False\n",
        "\n",
        "  def _convert_shape(input_shape):\n",
        "    input_shape = tensor_shape.TensorShape(input_shape)\n",
        "    if to_tuples:\n",
        "      input_shape = tuple(input_shape.as_list())\n",
        "    return input_shape\n",
        "\n",
        "  return map_structure_with_atomic(_is_atomic_shape, _convert_shape,\n",
        "                                   input_shape)\n",
        "\n",
        "\n",
        "class ListWrapper(object):\n",
        "  \"\"\"A wrapper for lists to be treated as elements for `nest`.\"\"\"\n",
        "\n",
        "  def __init__(self, list_to_wrap):\n",
        "    self._list = list_to_wrap\n",
        "\n",
        "  def as_list(self):\n",
        "    return self._list\n",
        "\n",
        "\n",
        "def convert_inner_node_data(nested, wrap=False):\n",
        "  \"\"\"Either wraps or unwraps innermost node data lists in `ListWrapper` objects.\n",
        "\n",
        "  Arguments:\n",
        "    nested: A nested data structure.\n",
        "    wrap: If `True`, wrap innermost lists in `ListWrapper` objects. If `False`,\n",
        "      unwraps `ListWrapper` objects into lists.\n",
        "\n",
        "  Returns:\n",
        "    Structure of same type as nested, with lists wrapped/unwrapped.\n",
        "  \"\"\"\n",
        "\n",
        "  def _is_serialized_node_data(nested):\n",
        "    # Node data can be of form `[layer_name, node_id, tensor_id]` or\n",
        "    # `[layer_name, node_id, tensor_id, kwargs]`.\n",
        "    if (isinstance(nested, list) and (len(nested) in [3, 4]) and\n",
        "        isinstance(nested[0], six.string_types)):\n",
        "      return True\n",
        "    return False\n",
        "\n",
        "  def _is_atomic_nested(nested):\n",
        "    \"\"\"Returns `True` if `nested` is a list representing node data.\"\"\"\n",
        "    if isinstance(nested, ListWrapper):\n",
        "      return True\n",
        "    if _is_serialized_node_data(nested):\n",
        "      return True\n",
        "    return not nest.is_sequence(nested)\n",
        "\n",
        "  def _convert_object_or_list(nested):\n",
        "    \"\"\"Convert b/t `ListWrapper` object and list representations.\"\"\"\n",
        "    if wrap:\n",
        "      if isinstance(nested, ListWrapper):\n",
        "        return nested\n",
        "      if _is_serialized_node_data(nested):\n",
        "        return ListWrapper(nested)\n",
        "      return nested\n",
        "    else:\n",
        "      if isinstance(nested, ListWrapper):\n",
        "        return nested.as_list()\n",
        "      return nested\n",
        "\n",
        "  return map_structure_with_atomic(_is_atomic_nested, _convert_object_or_list,\n",
        "                                   nested)\n",
        "\n",
        "\n",
        "def shape_type_conversion(fn):\n",
        "  \"\"\"Decorator that handles tuple/TensorShape conversion.\n",
        "\n",
        "  Used in `compute_output_shape` and `build`.\n",
        "\n",
        "  Arguments:\n",
        "    fn: function to wrap.\n",
        "\n",
        "  Returns:\n",
        "    Wrapped function.\n",
        "  \"\"\"\n",
        "\n",
        "  def wrapper(instance, input_shape):\n",
        "    # Pass shapes as tuples to `fn`\n",
        "    # This preserves compatibility with external Keras.\n",
        "    if input_shape is not None:\n",
        "      input_shape = convert_shapes(input_shape, to_tuples=True)\n",
        "    output_shape = fn(instance, input_shape)\n",
        "    # Return shapes from `fn` as TensorShapes.\n",
        "    if output_shape is not None:\n",
        "      output_shape = convert_shapes(output_shape, to_tuples=False)\n",
        "    return output_shape\n",
        "\n",
        "  return wrapper\n",
        "\n",
        "\n",
        "def are_all_symbolic_tensors(tensors):\n",
        "  return all(is_symbolic_tensor(tensor) for tensor in tensors)\n",
        "\n",
        "\n",
        "_user_convertible_tensor_types = set()\n",
        "\n",
        "\n",
        "def is_symbolic_tensor(tensor):\n",
        "  \"\"\"Returns whether a tensor is symbolic (from a TF graph) or an eager tensor.\n",
        "\n",
        "  A Variable can be seen as either: it is considered symbolic\n",
        "  when we are in a graph scope, and eager when we are in an eager scope.\n",
        "\n",
        "  Arguments:\n",
        "    tensor: A tensor instance to test.\n",
        "\n",
        "  Returns:\n",
        "    True for symbolic tensors, False for eager tensors.\n",
        "  \"\"\"\n",
        "  if isinstance(tensor, tuple(_user_convertible_tensor_types)):\n",
        "    tensor = ops.convert_to_tensor_or_composite(tensor)\n",
        "  if isinstance(tensor, variables.Variable):\n",
        "    # Variables that are output of a Keras Layer in Functional API mode\n",
        "    # should be considered symbolic.\n",
        "    # TODO(omalleyt): We need a better way to check this in order to\n",
        "    # enable `run_eagerly=True` for Models containing Layers that\n",
        "    # return Variables as outputs.\n",
        "    return (getattr(tensor, '_keras_history', False) or\n",
        "            not context.executing_eagerly())\n",
        "  if isinstance(tensor, composite_tensor.CompositeTensor):\n",
        "    component_tensors = nest.flatten(tensor, expand_composites=True)\n",
        "    return any(hasattr(t, 'graph') for t in component_tensors)\n",
        "  if isinstance(tensor, ops.Tensor):\n",
        "    return hasattr(tensor, 'graph')\n",
        "  return False\n",
        "\n",
        "\n",
        "def register_symbolic_tensor_type(cls):\n",
        "  \"\"\"Allows users to specify types regarded as symbolic `Tensor`s.\n",
        "\n",
        "  Used in conjunction with `tf.register_tensor_conversion_function`, calling\n",
        "  `tf.keras.utils.register_symbolic_tensor_type(cls)` allows non-`Tensor`\n",
        "  objects to be plumbed through Keras layers.\n",
        "\n",
        "  Example:\n",
        "\n",
        "  ```python\n",
        "  # One-time setup.\n",
        "  class Foo(object):\n",
        "    def __init__(self, input_):\n",
        "      self._input = input_\n",
        "    def value(self):\n",
        "      return tf.constant(42.)\n",
        "\n",
        "  tf.register_tensor_conversion_function(\n",
        "      Foo, lambda x, *args, **kwargs: x.value())\n",
        "\n",
        "  tf.keras.utils.register_symbolic_tensor_type(Foo)\n",
        "\n",
        "  # User-land.\n",
        "  layer = tf.keras.layers.Lambda(lambda input_: Foo(input_))\n",
        "  ```\n",
        "\n",
        "  Arguments:\n",
        "    cls: A `class` type which shall be regarded as a symbolic `Tensor`.\n",
        "  \"\"\"\n",
        "  global _user_convertible_tensor_types\n",
        "  _user_convertible_tensor_types.add(cls)\n",
        "\n",
        "\n",
        "def type_spec_from_value(value):\n",
        "  \"\"\"Grab type_spec without converting array-likes to tensors.\"\"\"\n",
        "  if isinstance(value, composite_tensor.CompositeTensor):\n",
        "    return value._type_spec  # pylint: disable=protected-access\n",
        "  # Get a TensorSpec for array-like data without\n",
        "  # converting the data to a Tensor\n",
        "  if hasattr(value, 'shape') and hasattr(value, 'dtype'):\n",
        "    return tensor_spec.TensorSpec(value.shape, value.dtype)\n",
        "  else:\n",
        "    return type_spec.type_spec_from_value(value)\n",
        "\n",
        "\n",
        "def is_tensor_or_variable(x):\n",
        "  return tensor_util.is_tensor(x) or isinstance(x, variables.Variable)\n",
        "\n",
        "\n",
        "def assert_no_legacy_layers(layers):\n",
        "  \"\"\"Prevent tf.layers.Layers from being used with Keras.\n",
        "\n",
        "  Certain legacy layers inherit from their keras analogs; however they are\n",
        "  not supported with keras and can lead to subtle and hard to diagnose bugs.\n",
        "\n",
        "  Args:\n",
        "    layers: A list of layers to check\n",
        "\n",
        "  Raises:\n",
        "    TypeError: If any elements of layers are tf.layers.Layers\n",
        "  \"\"\"\n",
        "\n",
        "  # isinstance check for tf.layers.Layer introduces a circular dependency.\n",
        "  legacy_layers = [l for l in layers if getattr(l, '_is_legacy_layer', None)]\n",
        "  if legacy_layers:\n",
        "    layer_str = '\\n'.join('  ' + str(l) for l in legacy_layers)\n",
        "    raise TypeError(\n",
        "        'The following are legacy tf.layers.Layers:\\n{}\\nTo use keras as a '\n",
        "        'framework (for instance using the Network, Model, or Sequential '\n",
        "        'classes), please use the tf.keras.layers implementation instead. '\n",
        "        '(Or, if writing custom layers, subclass from tf.keras.layers rather '\n",
        "        'than tf.layers)'.format(layer_str))\n",
        "\n",
        "\n",
        "@tf_contextlib.contextmanager\n",
        "def maybe_init_scope(layer):\n",
        "  \"\"\"Open an `init_scope` if in V2 mode and using the keras graph.\n",
        "\n",
        "  Arguments:\n",
        "    layer: The Layer/Model that is currently active.\n",
        "\n",
        "  Yields:\n",
        "    None\n",
        "  \"\"\"\n",
        "  # Don't open an init_scope in V1 mode or when using legacy tf.layers.\n",
        "  if (ops.executing_eagerly_outside_functions() and\n",
        "      getattr(layer, '_keras_style', True)):\n",
        "    with ops.init_scope():\n",
        "      yield\n",
        "  else:\n",
        "    yield\n",
        "\n",
        "\n",
        "@tf_contextlib.contextmanager\n",
        "def graph_context_for_symbolic_tensors(*args, **kwargs):\n",
        "  \"\"\"Returns graph context manager if any of the inputs is a symbolic tensor.\"\"\"\n",
        "  if any(is_symbolic_tensor(v) for v in list(args) + list(kwargs.values())):\n",
        "    with K.get_graph().as_default():\n",
        "      yield\n",
        "  else:\n",
        "    yield\n",
        "\n",
        "\n",
        "def dataset_is_infinite(dataset):\n",
        "  \"\"\"True if the passed dataset is infinite.\"\"\"\n",
        "  if ops.executing_eagerly_outside_functions():\n",
        "    return math_ops.equal(\n",
        "        cardinality.cardinality(dataset), cardinality.INFINITE)\n",
        "  else:\n",
        "    dataset_size = K.get_session().run(cardinality.cardinality(dataset))\n",
        "    return dataset_size == cardinality.INFINITE\n",
        "\n",
        "\n",
        "def get_tensor_spec(t, dynamic_batch=False, name=None):\n",
        "  \"\"\"Returns a `TensorSpec` given a single `Tensor` or `TensorSpec`.\"\"\"\n",
        "  if isinstance(t, type_spec.TypeSpec):\n",
        "    spec = t\n",
        "  elif isinstance(t, composite_tensor.CompositeTensor):\n",
        "    # TODO(b/148821952): Should these specs have a name attr?\n",
        "    spec = t._type_spec  # pylint: disable=protected-access\n",
        "  elif hasattr(t, 'shape') and hasattr(t, 'dtype'):\n",
        "    spec = tensor_spec.TensorSpec(shape=t.shape, dtype=t.dtype, name=name)\n",
        "  else:\n",
        "    return None  # Allow non-Tensors to pass through.\n",
        "\n",
        "  if not dynamic_batch:\n",
        "    return spec\n",
        "\n",
        "  dynamic_batch_spec = copy.deepcopy(spec)\n",
        "  # RaggedTensorSpec only has a private _shape.\n",
        "  shape = dynamic_batch_spec._shape.as_list()  # pylint: disable=protected-access\n",
        "  if shape:\n",
        "    shape[0] = None\n",
        "    dynamic_batch_spec._shape = tensor_shape.TensorShape(shape)  # pylint: disable=protected-access\n",
        "  return dynamic_batch_spec\n",
        "\n",
        "\n",
        "def to_numpy_or_python_type(tensors):\n",
        "  \"\"\"Converts a structure of `Tensor`s to `NumPy` arrays or Python scalar types.\n",
        "\n",
        "  For each tensor, it calls `tensor.numpy()`. If the result is a scalar value,\n",
        "  it converts it to a Python type, such as a float or int, by calling\n",
        "  `result.item()`.\n",
        "\n",
        "  Numpy scalars are converted, as Python types are often more convenient to deal\n",
        "  with. This is especially useful for bfloat16 Numpy scalars, which don't\n",
        "  support as many operations as other Numpy values.\n",
        "\n",
        "  Args:\n",
        "    tensors: A structure of tensors.\n",
        "\n",
        "  Returns:\n",
        "    `tensors`, but scalar tensors are converted to Python types and non-scalar\n",
        "    tensors are converted to Numpy arrays.\n",
        "  \"\"\"\n",
        "  def _to_single_numpy_or_python_type(t):\n",
        "    if isinstance(t, ops.Tensor):\n",
        "      x = t.numpy()\n",
        "      return x.item() if np.ndim(x) == 0 else x\n",
        "    return t  # Don't turn ragged or sparse tensors to NumPy.\n",
        "\n",
        "  return nest.map_structure(_to_single_numpy_or_python_type, tensors)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IDdhWl3JXaD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install tensorflow --upgrade --force-reinstall"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wHfsJ5nWLWh9",
        "colab": {}
      },
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import random\n",
        "import io\n",
        "import imageio\n",
        "import glob\n",
        "import scipy.misc\n",
        "import numpy as np\n",
        "from six import BytesIO\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from IPython.display import display, Javascript\n",
        "from IPython.display import Image as IPyImage\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import config_util\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "from object_detection.utils import colab_utils\n",
        "from object_detection.builders import model_builder\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wh_HPMOqWH9z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f0bd4ee0-c7e6-4ca9-ce88-7e338eec1dc0"
      },
      "source": [
        "#run model builder test\n",
        "!python /content/models/research/object_detection/builders/model_builder_tf2_test.py\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-09-21 05:55:28.412818: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Running tests under Python 3.6.9: /usr/bin/python3\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model\n",
            "2020-09-21 05:55:30.968603: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
            "2020-09-21 05:55:30.986209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-09-21 05:55:30.987182: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
            "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
            "2020-09-21 05:55:30.987232: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-21 05:55:30.989677: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-09-21 05:55:30.992305: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-09-21 05:55:30.992786: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-09-21 05:55:30.995168: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-09-21 05:55:30.996432: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-09-21 05:55:31.000977: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-09-21 05:55:31.001133: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-09-21 05:55:31.002066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-09-21 05:55:31.002933: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-09-21 05:55:31.003253: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2020-09-21 05:55:31.009033: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2200000000 Hz\n",
            "2020-09-21 05:55:31.009300: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x16b8d80 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-09-21 05:55:31.009339: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-09-21 05:55:31.103692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-09-21 05:55:31.104778: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x16b8f40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-09-21 05:55:31.104845: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
            "2020-09-21 05:55:31.105086: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-09-21 05:55:31.106062: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
            "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
            "2020-09-21 05:55:31.106117: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-21 05:55:31.106194: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-09-21 05:55:31.106241: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-09-21 05:55:31.106301: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-09-21 05:55:31.106344: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-09-21 05:55:31.106419: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-09-21 05:55:31.106483: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-09-21 05:55:31.106575: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-09-21 05:55:31.107570: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-09-21 05:55:31.108467: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-09-21 05:55:31.108521: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-21 05:55:31.868429: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-09-21 05:55:31.868507: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
            "2020-09-21 05:55:31.868526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
            "2020-09-21 05:55:31.868781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-09-21 05:55:31.869761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-09-21 05:55:31.870569: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2020-09-21 05:55:31.870630: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14951 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model): 1.16s\n",
            "I0921 05:55:31.889938 140293704972160 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_center_net_model): 1.16s\n",
            "[  FAILED  ] ModelBuilderTF2Test.test_create_center_net_model\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_experimental_model\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_experimental_model): 0.0s\n",
            "I0921 05:55:31.892559 140293704972160 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_experimental_model): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_experimental_model\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)): 0.05s\n",
            "I0921 05:55:31.942668 140293704972160 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)): 0.05s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)): 0.03s\n",
            "I0921 05:55:31.969339 140293704972160 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)): 0.03s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner): 0.03s\n",
            "I0921 05:55:31.997560 140293704972160 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner): 0.03s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul): 0.19s\n",
            "I0921 05:55:32.190302 140293704972160 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul): 0.19s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul): 0.17s\n",
            "I0921 05:55:32.365418 140293704972160 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul): 0.17s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul): 0.2s\n",
            "I0921 05:55:32.563982 140293704972160 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul): 0.2s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul): 0.19s\n",
            "I0921 05:55:32.753638 140293704972160 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul): 0.19s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_rfcn_model_from_config\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_rfcn_model_from_config): 0.19s\n",
            "I0921 05:55:32.942780 140293704972160 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_rfcn_model_from_config): 0.19s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_rfcn_model_from_config\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config): 0.05s\n",
            "I0921 05:55:32.997218 140293704972160 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config): 0.05s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_ssd_models_from_config\n",
            "I0921 05:55:33.358911 140293704972160 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet EfficientNet backbone version: efficientnet-b0\n",
            "I0921 05:55:33.359117 140293704972160 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num filters: 64\n",
            "I0921 05:55:33.359237 140293704972160 ssd_efficientnet_bifpn_feature_extractor.py:147] EfficientDet BiFPN num iterations: 3\n",
            "I0921 05:55:33.366324 140293704972160 efficientnet_model.py:148] round_filter input=32 output=32\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_ssd_models_from_config): 0.37s\n",
            "I0921 05:55:33.369549 140293704972160 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_ssd_models_from_config): 0.37s\n",
            "[  FAILED  ] ModelBuilderTF2Test.test_create_ssd_models_from_config\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update): 0.0s\n",
            "I0921 05:55:33.372782 140293704972160 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold): 0.0s\n",
            "I0921 05:55:33.374590 140293704972160 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_model_config_proto\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_model_config_proto): 0.0s\n",
            "I0921 05:55:33.375217 140293704972160 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_invalid_model_config_proto): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_model_config_proto\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_second_stage_batch_size\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_second_stage_batch_size): 0.0s\n",
            "I0921 05:55:33.376953 140293704972160 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_invalid_second_stage_batch_size): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_second_stage_batch_size\n",
            "[ RUN      ] ModelBuilderTF2Test.test_session\n",
            "[  SKIPPED ] ModelBuilderTF2Test.test_session\n",
            "[ RUN      ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor): 0.0s\n",
            "I0921 05:55:33.378605 140293704972160 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor\n",
            "[ RUN      ] ModelBuilderTF2Test.test_unknown_meta_architecture\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_meta_architecture): 0.0s\n",
            "I0921 05:55:33.379157 140293704972160 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_unknown_meta_architecture): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_unknown_meta_architecture\n",
            "[ RUN      ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_ssd_feature_extractor): 0.0s\n",
            "I0921 05:55:33.380271 140293704972160 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_unknown_ssd_feature_extractor): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor\n",
            "======================================================================\n",
            "ERROR: test_create_center_net_model (__main__.ModelBuilderTF2Test)\n",
            "ModelBuilderTF2Test.test_create_center_net_model\n",
            "Test building a CenterNet model from proto txt.\n",
            "----------------------------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/models/research/object_detection/builders/model_builder_tf2_test.py\", line 224, in test_create_center_net_model\n",
            "    model = model_builder.build(config, is_training=True)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/object_detection/builders/model_builder.py\", line 1088, in build\n",
            "    add_summaries)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/object_detection/builders/model_builder.py\", line 976, in _build_center_net_model\n",
            "    center_net_config.feature_extractor)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/object_detection/builders/model_builder.py\", line 1052, in _build_center_net_feature_extractor\n",
            "    bgr_ordering=feature_extractor_config.bgr_ordering\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/object_detection/models/center_net_resnet_feature_extractor.py\", line 146, in resnet_v2_101\n",
            "    bgr_ordering=bgr_ordering\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/object_detection/models/center_net_resnet_feature_extractor.py\", line 50, in __init__\n",
            "    include_top=False)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/applications/resnet_v2.py\", line 90, in ResNet101V2\n",
            "    classifier_activation=classifier_activation)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/applications/resnet.py\", line 170, in ResNet\n",
            "    padding=((3, 3), (3, 3)), name='conv1_pad')(img_input)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 926, in __call__\n",
            "    input_list)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 1098, in _functional_construction_call\n",
            "    self._maybe_build(inputs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 2630, in _maybe_build\n",
            "    input_shapes = tf_utils.get_shapes(inputs)\n",
            "AttributeError: module 'tensorflow.python.keras.utils.tf_utils' has no attribute 'get_shapes'\n",
            "\n",
            "======================================================================\n",
            "ERROR: test_create_ssd_models_from_config (__main__.ModelBuilderTF2Test)\n",
            "ModelBuilderTF2Test.test_create_ssd_models_from_config\n",
            "----------------------------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/object_detection/builders/model_builder_test.py\", line 212, in test_create_ssd_models_from_config\n",
            "    model = model_builder.build(model_proto, is_training=True)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/object_detection/builders/model_builder.py\", line 1088, in build\n",
            "    add_summaries)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/object_detection/builders/model_builder.py\", line 380, in _build_ssd_model\n",
            "    is_training=is_training)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/object_detection/builders/model_builder.py\", line 355, in _build_ssd_feature_extractor\n",
            "    return feature_extractor_class(**kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/object_detection/models/ssd_efficientnet_bifpn_feature_extractor.py\", line 312, in __init__\n",
            "    name=name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/object_detection/models/ssd_efficientnet_bifpn_feature_extractor.py\", line 163, in __init__\n",
            "    overrides={'rescale_input': False})\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/official/vision/image_classification/efficientnet/efficientnet_model.py\", line 498, in from_name\n",
            "    model = cls(config=config, overrides=overrides)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/official/vision/image_classification/efficientnet/efficientnet_model.py\", line 455, in __init__\n",
            "    output = efficientnet(image_input, self.config)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/official/vision/image_classification/efficientnet/efficientnet_model.py\", line 372, in efficientnet\n",
            "    name='stem')\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/official/vision/image_classification/efficientnet/efficientnet_model.py\", line 195, in conv2d_block\n",
            "    x = conv2d(**init_kwargs)(inputs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 926, in __call__\n",
            "    input_list)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 1098, in _functional_construction_call\n",
            "    self._maybe_build(inputs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 2630, in _maybe_build\n",
            "    input_shapes = tf_utils.get_shapes(inputs)\n",
            "AttributeError: module 'tensorflow.python.keras.utils.tf_utils' has no attribute 'get_shapes'\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 20 tests in 2.653s\n",
            "\n",
            "FAILED (errors=2, skipped=1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VA7Zbo3RLt3W",
        "colab": {}
      },
      "source": [
        "def load_image_into_numpy_array(path):\n",
        "  \"\"\"Load an image from file into a numpy array.\n",
        "\n",
        "  Puts image into numpy array to feed into tensorflow graph.\n",
        "  Note that by convention we put it into a numpy array with shape\n",
        "  (height, width, channels), where channels=3 for RGB.\n",
        "\n",
        "  Args:\n",
        "    path: a file path.\n",
        "\n",
        "  Returns:\n",
        "    uint8 numpy array with shape (img_height, img_width, 3)\n",
        "  \"\"\"\n",
        "  img_data = tf.io.gfile.GFile(path, 'rb').read()\n",
        "  image = Image.open(BytesIO(img_data))\n",
        "  (im_width, im_height) = image.size\n",
        "  return np.array(image.getdata()).reshape(\n",
        "      (im_height, im_width, 3)).astype(np.uint8)\n",
        "\n",
        "def plot_detections(image_np,\n",
        "                    boxes,\n",
        "                    classes,\n",
        "                    scores,\n",
        "                    category_index,\n",
        "                    figsize=(12, 16),\n",
        "                    image_name=None):\n",
        "  \"\"\"Wrapper function to visualize detections.\n",
        "\n",
        "  Args:\n",
        "    image_np: uint8 numpy array with shape (img_height, img_width, 3)\n",
        "    boxes: a numpy array of shape [N, 4]\n",
        "    classes: a numpy array of shape [N]. Note that class indices are 1-based,\n",
        "      and match the keys in the label map.\n",
        "    scores: a numpy array of shape [N] or None.  If scores=None, then\n",
        "      this function assumes that the boxes to be plotted are groundtruth\n",
        "      boxes and plot all boxes as black with no classes or scores.\n",
        "    category_index: a dict containing category dictionaries (each holding\n",
        "      category index `id` and category name `name`) keyed by category indices.\n",
        "    figsize: size for the figure.\n",
        "    image_name: a name for the image file.\n",
        "  \"\"\"\n",
        "  image_np_with_annotations = image_np.copy()\n",
        "  viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "      image_np_with_annotations,\n",
        "      boxes,\n",
        "      classes,\n",
        "      scores,\n",
        "      category_index,\n",
        "      use_normalized_coordinates=True,\n",
        "      min_score_thresh=0.8)\n",
        "  if image_name:\n",
        "    plt.imsave(image_name, image_np_with_annotations)\n",
        "  else:\n",
        "    plt.imshow(image_np_with_annotations)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IPbU4I7aL9Fl"
      },
      "source": [
        "# Prepare Tensorflow 2 Object Detection Training Data\n",
        "\n",
        "\n",
        "Roboflow automatically creates our TFRecord and label_map files that we need!\n",
        "\n",
        "**Generating your own TFRecords the only step you need to change for your own custom dataset.**\n",
        "\n",
        "Because we need one TFRecord file for our training data, and one TFRecord file for our test data, we'll create two separate datasets in Roboflow and generate one set of TFRecords for each.\n",
        "\n",
        "To create a dataset in Roboflow and generate TFRecords, follow [this step-by-step guide](https://blog.roboflow.ai/getting-started-with-roboflow/).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OIREg_YwDa7-"
      },
      "source": [
        "![](https://i.imgur.com/ZwMdcbY.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rk0Cdm2Nj0U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "30b158d8-6604-47b6-bc89-f166e9254c50"
      },
      "source": [
        "os.chdir('/content/models/research/deeplab/datasets')\n",
        "os.getcwd()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/models/research/deeplab/datasets'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QcHJuaurS_AO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "556d8f87-5720-4bb2-bb12-29bea3750b0f"
      },
      "source": [
        "#Downloading data from Roboflow\n",
        "#UPDATE THIS LINK - get our data from Roboflow\n",
        "%cd /content/models/research/deeplab/datasets\n",
        "!curl -L \"https://app.roboflow.ai/ds/Prdf49L1SN?key=mDsplij6yO\" > roboflow.zip; unzip roboflow.zip; rm roboflow.zip\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100    68  100    68    0     0    641      0 --:--:-- --:--:-- --:--:--   641\n",
            "100   891  100   891    0     0    912      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 83.6M  100 83.6M    0     0  41.6M      0  0:00:02  0:00:02 --:--:--  165M\n",
            "Archive:  roboflow.zip\n",
            "replace train/tomatoes-onions.tfrecord? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace test/tomatoes-onions.tfrecord? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YUd2wtfrqedy",
        "colab": {}
      },
      "source": [
        "# NOTE: Update these TFRecord names from \"cells\" and \"cells_label_map\" to your files!\n",
        "test_record_fname = './train/tomatoes-onions.tfrecord'\n",
        "train_record_fname = './test/tomatoes-onions.tfrecord'\n",
        "label_map_pbtxt_fname = './train/tomatoes-onions_label_map.pbtxt'"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2u1vqzBdOQbB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bb9fa983-48fa-4528-9070-f1f0e1a41cff"
      },
      "source": [
        "%cat /content/models/research/deeplab/deprecated/segmentation_dataset.py"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "# Lint as: python2, python3\n",
            "# Copyright 2018 The TensorFlow Authors All Rights Reserved.\n",
            "#\n",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "# you may not use this file except in compliance with the License.\n",
            "# You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "# ==============================================================================\n",
            "\n",
            "\"\"\"Provides data from semantic segmentation datasets.\n",
            "\n",
            "The SegmentationDataset class provides both images and annotations (semantic\n",
            "segmentation and/or instance segmentation) for TensorFlow. Currently, we\n",
            "support the following datasets:\n",
            "\n",
            "1. PASCAL VOC 2012 (http://host.robots.ox.ac.uk/pascal/VOC/voc2012/).\n",
            "\n",
            "PASCAL VOC 2012 semantic segmentation dataset annotates 20 foreground objects\n",
            "(e.g., bike, person, and so on) and leaves all the other semantic classes as\n",
            "one background class. The dataset contains 1464, 1449, and 1456 annotated\n",
            "images for the training, validation and test respectively.\n",
            "\n",
            "2. Cityscapes dataset (https://www.cityscapes-dataset.com)\n",
            "\n",
            "The Cityscapes dataset contains 19 semantic labels (such as road, person, car,\n",
            "and so on) for urban street scenes.\n",
            "\n",
            "3. ADE20K dataset (http://groups.csail.mit.edu/vision/datasets/ADE20K)\n",
            "\n",
            "The ADE20K dataset contains 150 semantic labels both urban street scenes and\n",
            "indoor scenes.\n",
            "\n",
            "References:\n",
            "  M. Everingham, S. M. A. Eslami, L. V. Gool, C. K. I. Williams, J. Winn,\n",
            "  and A. Zisserman, The pascal visual object classes challenge a retrospective.\n",
            "  IJCV, 2014.\n",
            "\n",
            "  M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson,\n",
            "  U. Franke, S. Roth, and B. Schiele, \"The cityscapes dataset for semantic urban\n",
            "  scene understanding,\" In Proc. of CVPR, 2016.\n",
            "\n",
            "  B. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso, A. Torralba, \"Scene Parsing\n",
            "  through ADE20K dataset\", In Proc. of CVPR, 2017.\n",
            "\"\"\"\n",
            "import collections\n",
            "import os.path\n",
            "import tensorflow as tf\n",
            "from tensorflow.contrib import slim as contrib_slim\n",
            "\n",
            "slim = contrib_slim\n",
            "\n",
            "dataset = slim.dataset\n",
            "\n",
            "tfexample_decoder = slim.tfexample_decoder\n",
            "\n",
            "\n",
            "_ITEMS_TO_DESCRIPTIONS = {\n",
            "    'image': 'A color image of varying height and width.',\n",
            "    'labels_class': ('A semantic segmentation label whose size matches image.'\n",
            "                     'Its values range from 0 (background) to num_classes.'),\n",
            "}\n",
            "\n",
            "# Named tuple to describe the dataset properties.\n",
            "DatasetDescriptor = collections.namedtuple(\n",
            "    'DatasetDescriptor',\n",
            "    ['splits_to_sizes',   # Splits of the dataset into training, val, and test.\n",
            "     'num_classes',   # Number of semantic classes, including the background\n",
            "                      # class (if exists). For example, there are 20\n",
            "                      # foreground classes + 1 background class in the PASCAL\n",
            "                      # VOC 2012 dataset. Thus, we set num_classes=21.\n",
            "     'ignore_label',  # Ignore label value.\n",
            "    ]\n",
            ")\n",
            "\n",
            "_CITYSCAPES_INFORMATION = DatasetDescriptor(\n",
            "    splits_to_sizes={\n",
            "        'train_fine': 2975,\n",
            "        'val_fine': 500,\n",
            "    },\n",
            "    num_classes=19,\n",
            "    ignore_label=255,\n",
            ")\n",
            "\n",
            "_PASCAL_VOC_SEG_INFORMATION = DatasetDescriptor(\n",
            "    splits_to_sizes={\n",
            "        'train': 1464,\n",
            "        'train_aug': 10582,\n",
            "        'trainval': 2913,\n",
            "        'val': 1449,\n",
            "    },\n",
            "    num_classes=21,\n",
            "    ignore_label=255,\n",
            ")\n",
            "\n",
            "_TOMATOES_ONIONS_INFORMATION = DatasetDescriptor(\n",
            "    split_to_sizes={\n",
            "        'train':79,\n",
            "        'val':21,\n",
            "    },\n",
            "    num_classes = 2,\n",
            "    ignore_label= 255,\n",
            ")\n",
            "\n",
            "# These number (i.e., 'train'/'test') seems to have to be hard coded\n",
            "# You are required to figure it out for your training/testing example.\n",
            "_ADE20K_INFORMATION = DatasetDescriptor(\n",
            "    splits_to_sizes={\n",
            "        'train': 20210,  # num of samples in images/training\n",
            "        'val': 2000,  # num of samples in images/validation\n",
            "    },\n",
            "    num_classes=151,\n",
            "    ignore_label=0,\n",
            ")\n",
            "\n",
            "\n",
            "_DATASETS_INFORMATION = {\n",
            "    'cityscapes': _CITYSCAPES_INFORMATION,\n",
            "    'pascal_voc_seg': _PASCAL_VOC_SEG_INFORMATION,\n",
            "    'ade20k': _ADE20K_INFORMATION,\n",
            "}\n",
            "\n",
            "# Default file pattern of TFRecord of TensorFlow Example.\n",
            "_FILE_PATTERN = '%s-*'\n",
            "\n",
            "\n",
            "def get_cityscapes_dataset_name():\n",
            "  return 'cityscapes'\n",
            "\n",
            "\n",
            "def get_dataset(dataset_name, split_name, dataset_dir):\n",
            "  \"\"\"Gets an instance of slim Dataset.\n",
            "\n",
            "  Args:\n",
            "    dataset_name: Dataset name.\n",
            "    split_name: A train/val Split name.\n",
            "    dataset_dir: The directory of the dataset sources.\n",
            "\n",
            "  Returns:\n",
            "    An instance of slim Dataset.\n",
            "\n",
            "  Raises:\n",
            "    ValueError: if the dataset_name or split_name is not recognized.\n",
            "  \"\"\"\n",
            "  if dataset_name not in _DATASETS_INFORMATION:\n",
            "    raise ValueError('The specified dataset is not supported yet.')\n",
            "\n",
            "  splits_to_sizes = _DATASETS_INFORMATION[dataset_name].splits_to_sizes\n",
            "\n",
            "  if split_name not in splits_to_sizes:\n",
            "    raise ValueError('data split name %s not recognized' % split_name)\n",
            "\n",
            "  # Prepare the variables for different datasets.\n",
            "  num_classes = _DATASETS_INFORMATION[dataset_name].num_classes\n",
            "  ignore_label = _DATASETS_INFORMATION[dataset_name].ignore_label\n",
            "\n",
            "  file_pattern = _FILE_PATTERN\n",
            "  file_pattern = os.path.join(dataset_dir, file_pattern % split_name)\n",
            "\n",
            "  # Specify how the TF-Examples are decoded.\n",
            "  keys_to_features = {\n",
            "      'image/encoded': tf.FixedLenFeature(\n",
            "          (), tf.string, default_value=''),\n",
            "      'image/filename': tf.FixedLenFeature(\n",
            "          (), tf.string, default_value=''),\n",
            "      'image/format': tf.FixedLenFeature(\n",
            "          (), tf.string, default_value='jpeg'),\n",
            "      'image/height': tf.FixedLenFeature(\n",
            "          (), tf.int64, default_value=0),\n",
            "      'image/width': tf.FixedLenFeature(\n",
            "          (), tf.int64, default_value=0),\n",
            "      'image/segmentation/class/encoded': tf.FixedLenFeature(\n",
            "          (), tf.string, default_value=''),\n",
            "      'image/segmentation/class/format': tf.FixedLenFeature(\n",
            "          (), tf.string, default_value='png'),\n",
            "  }\n",
            "  items_to_handlers = {\n",
            "      'image': tfexample_decoder.Image(\n",
            "          image_key='image/encoded',\n",
            "          format_key='image/format',\n",
            "          channels=3),\n",
            "      'image_name': tfexample_decoder.Tensor('image/filename'),\n",
            "      'height': tfexample_decoder.Tensor('image/height'),\n",
            "      'width': tfexample_decoder.Tensor('image/width'),\n",
            "      'labels_class': tfexample_decoder.Image(\n",
            "          image_key='image/segmentation/class/encoded',\n",
            "          format_key='image/segmentation/class/format',\n",
            "          channels=1),\n",
            "  }\n",
            "\n",
            "  decoder = tfexample_decoder.TFExampleDecoder(\n",
            "      keys_to_features, items_to_handlers)\n",
            "\n",
            "  return dataset.Dataset(\n",
            "      data_sources=file_pattern,\n",
            "      reader=tf.TFRecordReader,\n",
            "      decoder=decoder,\n",
            "      num_samples=splits_to_sizes[split_name],\n",
            "      items_to_descriptions=_ITEMS_TO_DESCRIPTIONS,\n",
            "      ignore_label=ignore_label,\n",
            "      num_classes=num_classes,\n",
            "      name=dataset_name,\n",
            "      multi_label=True)"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGMnTjTGRJ_l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "23fad853-17d5-4eae-f7fb-41bb051daac7"
      },
      "source": [
        "%%writefile /content/models/research/deeplab/deprecated/segmentation_dataset.py\n",
        "\n",
        "\n",
        "# Lint as: python2, python3\n",
        "# Copyright 2018 The TensorFlow Authors All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\n",
        "\"\"\"Provides data from semantic segmentation datasets.\n",
        "\n",
        "The SegmentationDataset class provides both images and annotations (semantic\n",
        "segmentation and/or instance segmentation) for TensorFlow. Currently, we\n",
        "support the following datasets:\n",
        "\n",
        "1. PASCAL VOC 2012 (http://host.robots.ox.ac.uk/pascal/VOC/voc2012/).\n",
        "\n",
        "PASCAL VOC 2012 semantic segmentation dataset annotates 20 foreground objects\n",
        "(e.g., bike, person, and so on) and leaves all the other semantic classes as\n",
        "one background class. The dataset contains 1464, 1449, and 1456 annotated\n",
        "images for the training, validation and test respectively.\n",
        "\n",
        "2. Cityscapes dataset (https://www.cityscapes-dataset.com)\n",
        "\n",
        "The Cityscapes dataset contains 19 semantic labels (such as road, person, car,\n",
        "and so on) for urban street scenes.\n",
        "\n",
        "3. ADE20K dataset (http://groups.csail.mit.edu/vision/datasets/ADE20K)\n",
        "\n",
        "The ADE20K dataset contains 150 semantic labels both urban street scenes and\n",
        "indoor scenes.\n",
        "\n",
        "References:\n",
        "  M. Everingham, S. M. A. Eslami, L. V. Gool, C. K. I. Williams, J. Winn,\n",
        "  and A. Zisserman, The pascal visual object classes challenge a retrospective.\n",
        "  IJCV, 2014.\n",
        "\n",
        "  M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson,\n",
        "  U. Franke, S. Roth, and B. Schiele, \"The cityscapes dataset for semantic urban\n",
        "  scene understanding,\" In Proc. of CVPR, 2016.\n",
        "\n",
        "  B. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso, A. Torralba, \"Scene Parsing\n",
        "  through ADE20K dataset\", In Proc. of CVPR, 2017.\n",
        "\"\"\"\n",
        "import collections\n",
        "import os.path\n",
        "import tensorflow as tf\n",
        "from tensorflow.contrib import slim as contrib_slim\n",
        "\n",
        "slim = contrib_slim\n",
        "\n",
        "dataset = slim.dataset\n",
        "\n",
        "tfexample_decoder = slim.tfexample_decoder\n",
        "\n",
        "\n",
        "_ITEMS_TO_DESCRIPTIONS = {\n",
        "    'image': 'A color image of varying height and width.',\n",
        "    'labels_class': ('A semantic segmentation label whose size matches image.'\n",
        "                     'Its values range from 0 (background) to num_classes.'),\n",
        "}\n",
        "\n",
        "# Named tuple to describe the dataset properties.\n",
        "DatasetDescriptor = collections.namedtuple(\n",
        "    'DatasetDescriptor',\n",
        "    ['splits_to_sizes',   # Splits of the dataset into training, val, and test.\n",
        "     'num_classes',   # Number of semantic classes, including the background\n",
        "                      # class (if exists). For example, there are 20\n",
        "                      # foreground classes + 1 background class in the PASCAL\n",
        "                      # VOC 2012 dataset. Thus, we set num_classes=21.\n",
        "     'ignore_label',  # Ignore label value.\n",
        "    ]\n",
        ")\n",
        "\n",
        "_CITYSCAPES_INFORMATION = DatasetDescriptor(\n",
        "    splits_to_sizes={\n",
        "        'train_fine': 2975,\n",
        "        'val_fine': 500,\n",
        "    },\n",
        "    num_classes=19,\n",
        "    ignore_label=255,\n",
        ")\n",
        "\n",
        "_PASCAL_VOC_SEG_INFORMATION = DatasetDescriptor(\n",
        "    splits_to_sizes={\n",
        "        'train': 1464,\n",
        "        'train_aug': 10582,\n",
        "        'trainval': 2913,\n",
        "        'val': 1449,\n",
        "    },\n",
        "    num_classes=21,\n",
        "    ignore_label=255,\n",
        ")\n",
        "\n",
        "_TOMATOES_ONIONS_INFORMATION = DatasetDescriptor(\n",
        "    split_to_sizes={\n",
        "        'train':79,\n",
        "        'val':21,\n",
        "    },\n",
        "    num_classes = 2,\n",
        "    ignore_label= 255,\n",
        ")\n",
        "\n",
        "# These number (i.e., 'train'/'test') seems to have to be hard coded\n",
        "# You are required to figure it out for your training/testing example.\n",
        "_ADE20K_INFORMATION = DatasetDescriptor(\n",
        "    splits_to_sizes={\n",
        "        'train': 20210,  # num of samples in images/training\n",
        "        'val': 2000,  # num of samples in images/validation\n",
        "    },\n",
        "    num_classes=151,\n",
        "    ignore_label=0,\n",
        ")\n",
        "\n",
        "\n",
        "_DATASETS_INFORMATION = {\n",
        "    'cityscapes': _CITYSCAPES_INFORMATION,\n",
        "    'pascal_voc_seg': _PASCAL_VOC_SEG_INFORMATION,\n",
        "    'ade20k': _ADE20K_INFORMATION,\n",
        "}\n",
        "\n",
        "# Default file pattern of TFRecord of TensorFlow Example.\n",
        "_FILE_PATTERN = '%s-*'\n",
        "\n",
        "\n",
        "def get_cityscapes_dataset_name():\n",
        "  return 'cityscapes'\n",
        "\n",
        "\n",
        "def get_dataset(dataset_name, split_name, dataset_dir):\n",
        "  \"\"\"Gets an instance of slim Dataset.\n",
        "\n",
        "  Args:\n",
        "    dataset_name: Dataset name.\n",
        "    split_name: A train/val Split name.\n",
        "    dataset_dir: The directory of the dataset sources.\n",
        "\n",
        "  Returns:\n",
        "    An instance of slim Dataset.\n",
        "\n",
        "  Raises:\n",
        "    ValueError: if the dataset_name or split_name is not recognized.\n",
        "  \"\"\"\n",
        "  if dataset_name not in _DATASETS_INFORMATION:\n",
        "    raise ValueError('The specified dataset is not supported yet.')\n",
        "\n",
        "  splits_to_sizes = _DATASETS_INFORMATION[dataset_name].splits_to_sizes\n",
        "\n",
        "  if split_name not in splits_to_sizes:\n",
        "    raise ValueError('data split name %s not recognized' % split_name)\n",
        "\n",
        "  # Prepare the variables for different datasets.\n",
        "  num_classes = _DATASETS_INFORMATION[dataset_name].num_classes\n",
        "  ignore_label = _DATASETS_INFORMATION[dataset_name].ignore_label\n",
        "\n",
        "  file_pattern = _FILE_PATTERN\n",
        "  file_pattern = os.path.join(dataset_dir, file_pattern % split_name)\n",
        "\n",
        "  # Specify how the TF-Examples are decoded.\n",
        "  keys_to_features = {\n",
        "      'image/encoded': tf.FixedLenFeature(\n",
        "          (), tf.string, default_value=''),\n",
        "      'image/filename': tf.FixedLenFeature(\n",
        "          (), tf.string, default_value=''),\n",
        "      'image/format': tf.FixedLenFeature(\n",
        "          (), tf.string, default_value='jpeg'),\n",
        "      'image/height': tf.FixedLenFeature(\n",
        "          (), tf.int64, default_value=0),\n",
        "      'image/width': tf.FixedLenFeature(\n",
        "          (), tf.int64, default_value=0),\n",
        "      'image/segmentation/class/encoded': tf.FixedLenFeature(\n",
        "          (), tf.string, default_value=''),\n",
        "      'image/segmentation/class/format': tf.FixedLenFeature(\n",
        "          (), tf.string, default_value='png'),\n",
        "  }\n",
        "  items_to_handlers = {\n",
        "      'image': tfexample_decoder.Image(\n",
        "          image_key='image/encoded',\n",
        "          format_key='image/format',\n",
        "          channels=3),\n",
        "      'image_name': tfexample_decoder.Tensor('image/filename'),\n",
        "      'height': tfexample_decoder.Tensor('image/height'),\n",
        "      'width': tfexample_decoder.Tensor('image/width'),\n",
        "      'labels_class': tfexample_decoder.Image(\n",
        "          image_key='image/segmentation/class/encoded',\n",
        "          format_key='image/segmentation/class/format',\n",
        "          channels=1),\n",
        "  }\n",
        "\n",
        "  decoder = tfexample_decoder.TFExampleDecoder(\n",
        "      keys_to_features, items_to_handlers)\n",
        "\n",
        "  return dataset.Dataset(\n",
        "      data_sources=file_pattern,\n",
        "      reader=tf.TFRecordReader,\n",
        "      decoder=decoder,\n",
        "      num_samples=splits_to_sizes[split_name],\n",
        "      items_to_descriptions=_ITEMS_TO_DESCRIPTIONS,\n",
        "      ignore_label=ignore_label,\n",
        "      num_classes=num_classes,\n",
        "      name=dataset_name,\n",
        "      multi_label=True)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting /content/models/research/deeplab/deprecated/segmentation_dataset.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gN0EUEa3e5Un",
        "colab": {}
      },
      "source": [
        "##change chosen model to deploy different models available in the TF2 object detection zoo\n",
        "MODELS_CONFIG = {\n",
        "    'efficientdet-d0': {\n",
        "        'model_name': 'efficientdet_d0_coco17_tpu-32',\n",
        "        'base_pipeline_file': 'ssd_efficientdet_d0_512x512_coco17_tpu-8.config',\n",
        "        'pretrained_checkpoint': 'efficientdet_d0_coco17_tpu-32.tar.gz',\n",
        "        'batch_size': 16\n",
        "    },\n",
        "    'efficientdet-d1': {\n",
        "        'model_name': 'efficientdet_d1_coco17_tpu-32',\n",
        "        'base_pipeline_file': 'ssd_efficientdet_d1_640x640_coco17_tpu-8.config',\n",
        "        'pretrained_checkpoint': 'efficientdet_d1_coco17_tpu-32.tar.gz',\n",
        "        'batch_size': 16\n",
        "    },\n",
        "    'efficientdet-d2': {\n",
        "        'model_name': 'efficientdet_d2_coco17_tpu-32',\n",
        "        'base_pipeline_file': 'ssd_efficientdet_d2_768x768_coco17_tpu-8.config',\n",
        "        'pretrained_checkpoint': 'efficientdet_d2_coco17_tpu-32.tar.gz',\n",
        "        'batch_size': 16\n",
        "    },\n",
        "        'efficientdet-d3': {\n",
        "        'model_name': 'efficientdet_d3_coco17_tpu-32',\n",
        "        'base_pipeline_file': 'ssd_efficientdet_d3_896x896_coco17_tpu-32.config',\n",
        "        'pretrained_checkpoint': 'efficientdet_d3_coco17_tpu-32.tar.gz',\n",
        "        'batch_size': 16\n",
        "    }\n",
        "}\n",
        "\n",
        "#in this tutorial we implement the lightweight, smallest state of the art efficientdet model\n",
        "#if you want to scale up tot larger efficientdet models you will likely need more compute!\n",
        "chosen_model = 'efficientdet-d0'\n",
        "\n",
        "num_steps = 40000 #The more steps, the longer the training. Increase if your loss function is still decreasing and validation metrics are increasing. \n",
        "num_eval_steps = 500 #Perform evaluation after so many steps\n",
        "\n",
        "model_name = MODELS_CONFIG[chosen_model]['model_name']\n",
        "pretrained_checkpoint = MODELS_CONFIG[chosen_model]['pretrained_checkpoint']\n",
        "base_pipeline_file = MODELS_CONFIG[chosen_model]['base_pipeline_file']\n",
        "batch_size = MODELS_CONFIG[chosen_model]['batch_size'] #if you can fit a large batch in memory, it may speed up your training"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kG4TmJUVrYQ7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "d4dcdb77-2284-4151-ece5-e990eebb096b"
      },
      "source": [
        "#download pretrained weights\n",
        "%mkdir /content/models/research/deploy/\n",
        "%cd /content/models/research/deploy/\n",
        "import tarfile\n",
        "download_tar = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/' + pretrained_checkpoint\n",
        "\n",
        "!wget {download_tar}\n",
        "tar = tarfile.open(pretrained_checkpoint)\n",
        "tar.extractall()\n",
        "tar.close()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/content/models/research/deploy/’: File exists\n",
            "/content/models/research/deploy\n",
            "--2020-09-21 05:56:34--  http://download.tensorflow.org/models/object_detection/tf2/20200711/efficientdet_d0_coco17_tpu-32.tar.gz\n",
            "Resolving download.tensorflow.org (download.tensorflow.org)... 74.125.195.128, 2607:f8b0:400e:c03::80\n",
            "Connecting to download.tensorflow.org (download.tensorflow.org)|74.125.195.128|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 30736482 (29M) [application/x-tar]\n",
            "Saving to: ‘efficientdet_d0_coco17_tpu-32.tar.gz.1’\n",
            "\n",
            "efficientdet_d0_coc 100%[===================>]  29.31M   139MB/s    in 0.2s    \n",
            "\n",
            "2020-09-21 05:56:34 (139 MB/s) - ‘efficientdet_d0_coco17_tpu-32.tar.gz.1’ saved [30736482/30736482]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrGp4T_FTyUC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "dc27432c-c953-46df-b1f4-e93e62578ae6"
      },
      "source": [
        "#download base training configuration file\n",
        "%cd /content/models/research/deploy\n",
        "download_config = 'https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/configs/tf2/' + base_pipeline_file\n",
        "!wget {download_config}"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/models/research/deploy\n",
            "--2020-09-21 05:56:35--  https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/configs/tf2/ssd_efficientdet_d0_512x512_coco17_tpu-8.config\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4630 (4.5K) [text/plain]\n",
            "Saving to: ‘ssd_efficientdet_d0_512x512_coco17_tpu-8.config.1’\n",
            "\n",
            "ssd_efficientdet_d0 100%[===================>]   4.52K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-09-21 05:56:35 (61.0 MB/s) - ‘ssd_efficientdet_d0_512x512_coco17_tpu-8.config.1’ saved [4630/4630]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h27PZDRqSgJH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pipeline_fname = '/content/models/research/deploy/' + base_pipeline_file\n",
        "fine_tune_checkpoint = '/content/models/research/deploy/' + model_name + '/checkpoint/ckpt-0'\n",
        "\n",
        "PATH_TO_INITIAL_CHECKPOINT = fine_tune_checkpoint"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-y_MzVON71v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install -U tensorflow_estimator --force-reinstall\n",
        "#!pip install 'tensorflow-estimator<1.15.0rc0,>=1.14.0rc0' --force-reinstall"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zHhV9eLSly8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "86b903cc-8bb1-43ff-f40a-4f9001e7f160"
      },
      "source": [
        "%cd /content/models/research/\n",
        "!python3 deeplab/train.py \\\n",
        "   --logtostderr \\\n",
        "   --training_number_of_steps=1400 \\\n",
        "   --train_split=\"train\" \\\n",
        "   --model_variant=\"xception_65\" \\\n",
        "   --atrous_rates=6 \\\n",
        "   --atrous_rates=12 \\\n",
        "   --atrous_rates=18 \\\n",
        "   --output_stride=16 \\\n",
        "   --decoder_output_stride=4 \\\n",
        "   --train_crop_size=769 \\\n",
        "   --train_crop_size=769 \\\n",
        "   --train_batch_size=1 \\\n",
        "   --dataset=\"cityscapes\" \\\n",
        "   --tf_initial_checkpoint=${PATH_TO_INITIAL_CHECKPOINT} \\"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/models/research\n",
            "2020-09-21 05:56:36.761112: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Traceback (most recent call last):\n",
            "  File \"deeplab/train.py\", line 26, in <module>\n",
            "    from tensorflow.contrib import quantize as contrib_quantize\n",
            "ModuleNotFoundError: No module named 'tensorflow.contrib'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpiRABa5WZT0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfsJIsTvWZZp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcuFdsA0WZih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dvo_z9iFWZph",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCOH0OwQWZxR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V46fk-FQWZtt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhcOf9_UWZmY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqHmvvJNWZgA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kL8d1N7OWZd8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4JC9OsCWZXG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "I2MAcgJ53STW"
      },
      "source": [
        "# Configure Custom TensorFlow2 Object Detection Training Configuration\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> In this section you can specify any model in the [TF2 OD model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md) and set up your training configuration.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b_ki9jOqxn7V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "outputId": "a9baa155-3cf5-4e81-98a4-ede007ecacdb"
      },
      "source": [
        "#prepare\n",
        "pipeline_fname = '/content/models/research/deploy/' + base_pipeline_file\n",
        "fine_tune_checkpoint = '/content/models/research/deploy/' + model_name + '/checkpoint/ckpt-0'\n",
        "\n",
        "def get_num_classes(pbtxt_fname):\n",
        "    from object_detection.utils import label_map_util\n",
        "    label_map = label_map_util.load_labelmap(pbtxt_fname)\n",
        "    categories = label_map_util.convert_label_map_to_categories(\n",
        "        label_map, max_num_classes=90, use_display_name=True)\n",
        "    category_index = label_map_util.create_category_index(categories)\n",
        "    return len(category_index.keys())\n",
        "num_classes = get_num_classes(label_map_pbtxt_fname)\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-2ba4fa691ce2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mcategory_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_map_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_category_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategory_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_num_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_map_pbtxt_fname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-21-2ba4fa691ce2>\u001b[0m in \u001b[0;36mget_num_classes\u001b[0;34m(pbtxt_fname)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_num_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpbtxt_fname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mobject_detection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlabel_map_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mlabel_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_map_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_labelmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpbtxt_fname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     categories = label_map_util.convert_label_map_to_categories(\n\u001b[1;32m      9\u001b[0m         label_map, max_num_classes=90, use_display_name=True)\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/object_detection/utils/label_map_util.py\u001b[0m in \u001b[0;36mload_labelmap\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    154\u001b[0m   \"\"\"\n\u001b[1;32m    155\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0mlabel_map_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m     \u001b[0mlabel_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstring_int_label_map_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStringIntLabelMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    114\u001b[0m       \u001b[0mstring\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstring\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mregular\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \"\"\"\n\u001b[0;32m--> 116\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preread_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m       \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36m_preread_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     77\u001b[0m                                            \"File isn't open for reading\")\n\u001b[1;32m     78\u001b[0m       self._read_buf = _pywrap_file_io.BufferedInputStream(\n\u001b[0;32m---> 79\u001b[0;31m           self.__name, 1024 * 512)\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_prewrite_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: ./train/tomatoes-onions_label_map.pbtxt; No such file or directory"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5eA5ht3_yukT",
        "colab": {}
      },
      "source": [
        "#write custom configuration file by slotting our dataset, model checkpoint, and training parameters into the base pipeline file\n",
        "\n",
        "import re\n",
        "\n",
        "%cd /content/models/research/deploy\n",
        "print('writing custom configuration file')\n",
        "\n",
        "with open(pipeline_fname) as f:\n",
        "    s = f.read()\n",
        "with open('pipeline_file.config', 'w') as f:\n",
        "    \n",
        "    # fine_tune_checkpoint\n",
        "    s = re.sub('fine_tune_checkpoint: \".*?\"',\n",
        "               'fine_tune_checkpoint: \"{}\"'.format(fine_tune_checkpoint), s)\n",
        "    \n",
        "    # tfrecord files train and test.\n",
        "    s = re.sub(\n",
        "        '(input_path: \".*?)(PATH_TO_BE_CONFIGURED/train)(.*?\")', 'input_path: \"{}\"'.format(train_record_fname), s)\n",
        "    s = re.sub(\n",
        "        '(input_path: \".*?)(PATH_TO_BE_CONFIGURED/val)(.*?\")', 'input_path: \"{}\"'.format(test_record_fname), s)\n",
        "\n",
        "    # label_map_path\n",
        "    s = re.sub(\n",
        "        'label_map_path: \".*?\"', 'label_map_path: \"{}\"'.format(label_map_pbtxt_fname), s)\n",
        "\n",
        "    # Set training batch_size.\n",
        "    s = re.sub('batch_size: [0-9]+',\n",
        "               'batch_size: {}'.format(batch_size), s)\n",
        "\n",
        "    # Set training steps, num_steps\n",
        "    s = re.sub('num_steps: [0-9]+',\n",
        "               'num_steps: {}'.format(num_steps), s)\n",
        "    \n",
        "    # Set number of classes num_classes.\n",
        "    s = re.sub('num_classes: [0-9]+',\n",
        "               'num_classes: {}'.format(num_classes), s)\n",
        "    \n",
        "    #fine-tune checkpoint type\n",
        "    s = re.sub(\n",
        "        'fine_tune_checkpoint_type: \"classification\"', 'fine_tune_checkpoint_type: \"{}\"'.format('detection'), s)\n",
        "        \n",
        "    f.write(s)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HEsOLOMHzBqF",
        "colab": {}
      },
      "source": [
        "%cat /content/models/research/deploy/pipeline_file.config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GMlaN3rs3zLe",
        "colab": {}
      },
      "source": [
        "pipeline_file = '/content/models/research/deploy/pipeline_file.config'\n",
        "model_dir = '/content/training/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XxPj_QV43qD5"
      },
      "source": [
        "# Train Custom TF2 Object Detector\n",
        "\n",
        "* pipeline_file: defined above in writing custom training configuration\n",
        "* model_dir: the location tensorboard logs and saved model checkpoints will save to\n",
        "* num_train_steps: how long to train for\n",
        "* num_eval_steps: perform eval on validation set after this many steps\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tQTfZChVzzpZ",
        "colab": {}
      },
      "source": [
        "!python /content/models/research/object_detection/model_main_tf2.py \\\n",
        "    --pipeline_config_path={pipeline_file} \\\n",
        "    --model_dir={model_dir} \\\n",
        "    --alsologtostderr \\\n",
        "    --num_train_steps={num_steps} \\\n",
        "    --sample_1_of_n_eval_examples=1 \\\n",
        "    --num_eval_steps={num_eval_steps}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9KNv1N_hUibE",
        "colab": {}
      },
      "source": [
        "#run model evaluation to obtain performance metrics\n",
        "#!python /content/models/research/object_detection/model_main_tf2.py \\\n",
        "    #--pipeline_config_path={pipeline_file} \\\n",
        "    #--model_dir={model_dir} \\\n",
        "    #--checkpoint_dir={model_dir} \\\n",
        "#Not yet implemented for EfficientDet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TI9iCCxoNlAL",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir '/content/training/train'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QXZ5RxipUhWF",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4Vk2146Ogil3"
      },
      "source": [
        "## Exporting a Trained Inference Graph\n",
        "Still to come for TF2 models, we will be updating this Colab notebook accordingly as the functionality is added. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vqaZ4v-vIuDl",
        "colab": {}
      },
      "source": [
        "#see where our model saved weights\n",
        "%ls '/content/training/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YnSEZIzl4M10",
        "colab": {}
      },
      "source": [
        "#run conversion script\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "output_directory = '/content/fine_tuned_model'\n",
        "\n",
        "#place the model weights you would like to export here\n",
        "last_model_path = '/content/training/'\n",
        "print(last_model_path)\n",
        "!python /content/models/research/object_detection/exporter_main_v2.py \\\n",
        "    --trained_checkpoint_dir {last_model_path} \\\n",
        "    --output_directory {output_directory} \\\n",
        "    --pipeline_config_path {pipeline_file}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TsE_uVjlsz3u",
        "colab": {}
      },
      "source": [
        "%ls '/content/fine_tuned_model/saved_model/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7Vz2vJeCCyZR"
      },
      "source": [
        "# Run Inference on Test Images with Custom TensorFlow2 Object Detector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kcR4PWC3KBau",
        "colab": {}
      },
      "source": [
        "#downloading test images from Roboflow\n",
        "#export dataset above with format COCO JSON\n",
        "#or import your test images via other means. \n",
        "%mkdir /content/test/\n",
        "%cd /content/test/\n",
        "!curl -L \"[YOUR LINK HERE]\" > roboflow.zip; unzip roboflow.zip; rm roboflow.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xxtm1NutE5vK",
        "colab": {}
      },
      "source": [
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import io\n",
        "import scipy.misc\n",
        "import numpy as np\n",
        "from six import BytesIO\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import config_util\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "from object_detection.builders import model_builder\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qs1HJnEhyevJ",
        "colab": {}
      },
      "source": [
        "def load_image_into_numpy_array(path):\n",
        "  \"\"\"Load an image from file into a numpy array.\n",
        "\n",
        "  Puts image into numpy array to feed into tensorflow graph.\n",
        "  Note that by convention we put it into a numpy array with shape\n",
        "  (height, width, channels), where channels=3 for RGB.\n",
        "\n",
        "  Args:\n",
        "    path: the file path to the image\n",
        "\n",
        "  Returns:\n",
        "    uint8 numpy array with shape (img_height, img_width, 3)\n",
        "  \"\"\"\n",
        "  img_data = tf.io.gfile.GFile(path, 'rb').read()\n",
        "  image = Image.open(BytesIO(img_data))\n",
        "  (im_width, im_height) = image.size\n",
        "  return np.array(image.getdata()).reshape(\n",
        "      (im_height, im_width, 3)).astype(np.uint8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0f6DTolSDfXs",
        "colab": {}
      },
      "source": [
        "%ls '/content/training/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gFY75DfTDHaU",
        "colab": {}
      },
      "source": [
        "#recover our saved model\n",
        "pipeline_config = pipeline_file\n",
        "#generally you want to put the last ckpt from training in here\n",
        "model_dir = '/content/training/ckpt-11'\n",
        "configs = config_util.get_configs_from_pipeline_file(pipeline_config)\n",
        "model_config = configs['model']\n",
        "detection_model = model_builder.build(\n",
        "      model_config=model_config, is_training=False)\n",
        "\n",
        "# Restore checkpoint\n",
        "ckpt = tf.compat.v2.train.Checkpoint(\n",
        "      model=detection_model)\n",
        "ckpt.restore(os.path.join('/content/training/ckpt-19'))\n",
        "\n",
        "\n",
        "def get_model_detection_function(model):\n",
        "  \"\"\"Get a tf.function for detection.\"\"\"\n",
        "\n",
        "  @tf.function\n",
        "  def detect_fn(image):\n",
        "    \"\"\"Detect objects in image.\"\"\"\n",
        "\n",
        "    image, shapes = model.preprocess(image)\n",
        "    prediction_dict = model.predict(image, shapes)\n",
        "    detections = model.postprocess(prediction_dict, shapes)\n",
        "\n",
        "    return detections, prediction_dict, tf.reshape(shapes, [-1])\n",
        "\n",
        "  return detect_fn\n",
        "\n",
        "detect_fn = get_model_detection_function(detection_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-Ycfl7rnDT1D",
        "colab": {}
      },
      "source": [
        "#map labels for inference decoding\n",
        "label_map_path = configs['eval_input_config'].label_map_path\n",
        "label_map = label_map_util.load_labelmap(label_map_path)\n",
        "categories = label_map_util.convert_label_map_to_categories(\n",
        "    label_map,\n",
        "    max_num_classes=label_map_util.get_max_label_map_index(label_map),\n",
        "    use_display_name=True)\n",
        "category_index = label_map_util.create_category_index(categories)\n",
        "label_map_dict = label_map_util.get_label_map_dict(label_map, use_display_name=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wN1BzORoIzV4",
        "colab": {}
      },
      "source": [
        "#run detector on test image\n",
        "#it takes a little longer on the first run and then runs at normal speed. \n",
        "import random\n",
        "\n",
        "TEST_IMAGE_PATHS = glob.glob('/content/test/test/*.jpg')\n",
        "image_path = random.choice(TEST_IMAGE_PATHS)\n",
        "image_np = load_image_into_numpy_array(image_path)\n",
        "\n",
        "# Things to try:\n",
        "# Flip horizontally\n",
        "# image_np = np.fliplr(image_np).copy()\n",
        "\n",
        "# Convert image to grayscale\n",
        "# image_np = np.tile(\n",
        "#     np.mean(image_np, 2, keepdims=True), (1, 1, 3)).astype(np.uint8)\n",
        "\n",
        "input_tensor = tf.convert_to_tensor(\n",
        "    np.expand_dims(image_np, 0), dtype=tf.float32)\n",
        "detections, predictions_dict, shapes = detect_fn(input_tensor)\n",
        "\n",
        "label_id_offset = 1\n",
        "image_np_with_detections = image_np.copy()\n",
        "\n",
        "viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "      image_np_with_detections,\n",
        "      detections['detection_boxes'][0].numpy(),\n",
        "      (detections['detection_classes'][0].numpy() + label_id_offset).astype(int),\n",
        "      detections['detection_scores'][0].numpy(),\n",
        "      category_index,\n",
        "      use_normalized_coordinates=True,\n",
        "      max_boxes_to_draw=200,\n",
        "      min_score_thresh=.5,\n",
        "      agnostic_mode=False,\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(12,16))\n",
        "plt.imshow(image_np_with_detections)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zQ-N94cKB82o"
      },
      "source": [
        "## Congrats!\n",
        "\n",
        "Hope you enjoyed this!\n",
        "\n",
        "--Team [Roboflow](https://roboflow.ai)\n"
      ]
    }
  ]
}